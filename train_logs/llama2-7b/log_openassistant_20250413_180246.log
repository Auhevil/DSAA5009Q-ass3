INFO 04-13 18:02:52 __init__.py:190] Automatically detected platform cuda.
[INFO|2025-04-13 18:02:54] llamafactory.cli:143 >> Initializing 1 distributed tasks at: 127.0.0.1:29491
[WARNING|2025-04-13 18:03:00] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-04-13 18:03:00] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,582 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,582 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,583 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,583 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,583 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,583 >> loading file chat_template.jinja
[INFO|configuration_utils.py:697] 2025-04-13 18:03:00,669 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 18:03:00,669 >> Model config LlamaConfig {
  "_name_or_path": "/data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,670 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,670 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,670 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,670 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,670 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 18:03:00,670 >> loading file chat_template.jinja
[INFO|2025-04-13 18:03:00] llamafactory.data.template:143 >> Add pad token: </s>
[INFO|2025-04-13 18:03:00] llamafactory.data.loader:143 >> Loading dataset openassistant.json...
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3200 examples [00:00, 30224.11 examples/s]Generating train split: 3200 examples [00:00, 30021.50 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  19%|█▉        | 189/1000 [00:00<00:00, 1827.86 examples/s]Converting format of dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:00<00:00, 4457.85 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 3195.06 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:04, 196.57 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 252/1000 [00:00<00:01, 632.98 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 441/1000 [00:00<00:00, 984.12 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:00<00:00, 1096.69 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:00<00:00, 1380.57 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:01<00:00, 988.63 examples/s]
training example:
input_ids:
[1, 518, 25580, 29962, 376, 7185, 459, 1100, 29891, 29908, 14637, 304, 263, 9999, 3829, 988, 727, 338, 871, 697, 1321, 7598, 363, 263, 3153, 1781, 470, 2669, 29889, 512, 7766, 1199, 29892, 445, 1840, 338, 10734, 8018, 297, 278, 10212, 9999, 29892, 988, 263, 1601, 459, 1100, 29891, 5703, 261, 756, 7282, 3081, 975, 278, 281, 1179, 322, 1985, 5855, 310, 1009, 22873, 29889, 450, 10122, 310, 263, 1601, 459, 1100, 29891, 508, 1121, 297, 5224, 281, 1179, 322, 12212, 5703, 358, 28602, 1907, 363, 17162, 29892, 408, 278, 5703, 261, 756, 2217, 297, 1760, 573, 304, 7910, 281, 1179, 470, 3867, 2253, 1985, 5855, 29889, 13, 13, 4789, 296, 5925, 756, 15659, 7037, 1601, 459, 1100, 583, 297, 6397, 2722, 1316, 408, 3240, 737, 322, 5172, 9687, 29892, 988, 263, 2846, 2919, 14582, 2761, 263, 7282, 11910, 310, 278, 9999, 313, 29933, 440, 575, 669, 341, 728, 295, 29892, 29871, 29906, 29900, 29896, 29941, 467, 512, 1438, 6397, 2722, 29892, 17162, 4049, 3700, 4482, 281, 1179, 29892, 9078, 23633, 29892, 322, 12212, 289, 1191, 17225, 3081, 29892, 8236, 304, 263, 6434, 988, 896, 526, 14278, 373, 278, 5703, 261, 363, 1009, 7294, 22342, 29889, 910, 26307, 508, 1121, 297, 4340, 1462, 23881, 310, 281, 1179, 322, 263, 4845, 457, 297, 1985, 5855, 29889, 13, 13, 3563, 497, 29892, 278, 6964, 310, 1601, 459, 1100, 29891, 338, 18853, 304, 8004, 278, 19753, 310, 10212, 2791, 1691, 322, 278, 10879, 310, 9999, 3081, 373, 17162, 29889, 8725, 5925, 338, 4312, 304, 2274, 278, 15834, 322, 10879, 310, 1601, 459, 1100, 583, 373, 278, 26504, 322, 304, 2693, 24833, 304, 3211, 445, 2228, 29889, 13, 13, 1123, 10662, 29901, 13, 29933, 440, 575, 29892, 435, 1696, 669, 341, 728, 295, 29892, 365, 29889, 313, 29906, 29900, 29896, 29941, 467, 450, 14617, 310, 12767, 403, 11080, 329, 3145, 322, 4231, 273, 1455, 6175, 1211, 1338, 408, 7298, 5084, 310, 390, 1237, 297, 7488, 29871, 29896, 2431, 1760, 512, 26807, 29889, 8237, 310, 12884, 293, 9034, 1103, 3145, 29892, 29871, 29906, 29955, 29898, 29941, 511, 29871, 29945, 29955, 29899, 29955, 29947, 29889, 518, 29914, 25580, 29962, 1815, 366, 2436, 263, 3273, 18707, 1048, 278, 29527, 749, 310, 278, 1840, 376, 3712, 459, 1100, 29891, 29908, 297, 7766, 1199, 29973, 3529, 671, 6455, 4475, 304, 7037, 1601, 459, 1100, 583, 297, 278, 23390, 9999, 322, 274, 568, 8018, 5925, 29889, 2]
inputs:
<s> [INST] "Monopsony" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.

Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.

Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.

References:
Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78. [/INST] Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1815, 366, 2436, 263, 3273, 18707, 1048, 278, 29527, 749, 310, 278, 1840, 376, 3712, 459, 1100, 29891, 29908, 297, 7766, 1199, 29973, 3529, 671, 6455, 4475, 304, 7037, 1601, 459, 1100, 583, 297, 278, 23390, 9999, 322, 274, 568, 8018, 5925, 29889, 2]
labels:
Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</s>
[INFO|configuration_utils.py:697] 2025-04-13 18:03:03,413 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 18:03:03,414 >> Model config LlamaConfig {
  "_name_or_path": "/data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3979] 2025-04-13 18:03:03,483 >> loading weights file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1633] 2025-04-13 18:03:03,483 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1140] 2025-04-13 18:03:03,485 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]
[INFO|modeling_utils.py:4970] 2025-04-13 18:03:09,424 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4978] 2025-04-13 18:03:09,424 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1093] 2025-04-13 18:03:09,510 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:1140] 2025-04-13 18:03:09,510 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|2025-04-13 18:03:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-04-13 18:03:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-04-13 18:03:09] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-04-13 18:03:09] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-04-13 18:03:09] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,gate_proj,v_proj,q_proj,up_proj,o_proj,k_proj
[INFO|2025-04-13 18:03:09] llamafactory.model.loader:143 >> trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:746] 2025-04-13 18:03:10,013 >> Using auto half precision backend
[WARNING|trainer.py:781] 2025-04-13 18:03:10,014 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[INFO|trainer.py:2405] 2025-04-13 18:03:10,967 >> ***** Running training *****
[INFO|trainer.py:2406] 2025-04-13 18:03:10,967 >>   Num examples = 1,000
[INFO|trainer.py:2407] 2025-04-13 18:03:10,967 >>   Num Epochs = 3
[INFO|trainer.py:2408] 2025-04-13 18:03:10,967 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2411] 2025-04-13 18:03:10,967 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2412] 2025-04-13 18:03:10,967 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2413] 2025-04-13 18:03:10,967 >>   Total optimization steps = 93
[INFO|trainer.py:2414] 2025-04-13 18:03:10,972 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/93 [00:00<?, ?it/s]  1%|          | 1/93 [00:08<12:16,  8.01s/it]  2%|▏         | 2/93 [00:14<10:58,  7.23s/it]  3%|▎         | 3/93 [00:21<10:23,  6.93s/it]  4%|▍         | 4/93 [00:28<10:11,  6.87s/it]  5%|▌         | 5/93 [00:33<09:32,  6.51s/it]  6%|▋         | 6/93 [00:39<08:57,  6.18s/it]  8%|▊         | 7/93 [00:47<09:47,  6.83s/it]  9%|▊         | 8/93 [00:54<09:50,  6.95s/it] 10%|▉         | 9/93 [01:01<09:36,  6.87s/it] 11%|█         | 10/93 [01:07<08:57,  6.48s/it]                                               {'loss': 1.9084, 'grad_norm': 0.4231013059616089, 'learning_rate': 1e-05, 'epoch': 0.32}
 11%|█         | 10/93 [01:07<08:57,  6.48s/it] 12%|█▏        | 11/93 [01:14<09:19,  6.82s/it] 13%|█▎        | 12/93 [01:23<09:49,  7.27s/it] 14%|█▍        | 13/93 [01:28<09:06,  6.83s/it] 15%|█▌        | 14/93 [01:35<09:00,  6.84s/it] 16%|█▌        | 15/93 [01:42<09:01,  6.94s/it] 17%|█▋        | 16/93 [01:48<08:30,  6.63s/it] 18%|█▊        | 17/93 [01:54<07:58,  6.30s/it] 19%|█▉        | 18/93 [02:02<08:36,  6.89s/it] 20%|██        | 19/93 [02:10<08:45,  7.10s/it] 22%|██▏       | 20/93 [02:16<08:17,  6.81s/it]                                               {'loss': 1.8827, 'grad_norm': 0.5914782881736755, 'learning_rate': 8.795180722891567e-06, 'epoch': 0.64}
 22%|██▏       | 20/93 [02:16<08:17,  6.81s/it] 23%|██▎       | 21/93 [02:22<07:51,  6.55s/it] 24%|██▎       | 22/93 [02:29<08:02,  6.80s/it] 25%|██▍       | 23/93 [02:36<07:58,  6.84s/it] 26%|██▌       | 24/93 [02:42<07:36,  6.61s/it] 27%|██▋       | 25/93 [02:47<06:46,  5.97s/it] 28%|██▊       | 26/93 [02:53<06:51,  6.13s/it] 29%|██▉       | 27/93 [02:59<06:42,  6.09s/it] 30%|███       | 28/93 [03:05<06:26,  5.95s/it] 31%|███       | 29/93 [03:13<06:59,  6.56s/it] 32%|███▏      | 30/93 [03:20<07:15,  6.91s/it]                                               {'loss': 1.9472, 'grad_norm': 0.601430356502533, 'learning_rate': 7.590361445783133e-06, 'epoch': 0.96}
 32%|███▏      | 30/93 [03:20<07:15,  6.91s/it] 33%|███▎      | 31/93 [03:27<06:53,  6.66s/it] 34%|███▍      | 32/93 [03:28<05:10,  5.09s/it] 35%|███▌      | 33/93 [03:36<06:04,  6.07s/it] 37%|███▋      | 34/93 [03:44<06:34,  6.69s/it] 38%|███▊      | 35/93 [03:51<06:23,  6.60s/it] 39%|███▊      | 36/93 [03:57<06:16,  6.60s/it] 40%|███▉      | 37/93 [04:04<06:09,  6.60s/it] 41%|████      | 38/93 [04:10<05:46,  6.30s/it] 42%|████▏     | 39/93 [04:14<05:15,  5.84s/it] 43%|████▎     | 40/93 [04:21<05:21,  6.06s/it]                                               {'loss': 1.8419, 'grad_norm': 0.52719646692276, 'learning_rate': 6.385542168674699e-06, 'epoch': 1.26}
 43%|████▎     | 40/93 [04:21<05:21,  6.06s/it] 44%|████▍     | 41/93 [04:29<05:42,  6.58s/it] 45%|████▌     | 42/93 [04:35<05:23,  6.35s/it] 46%|████▌     | 43/93 [04:42<05:28,  6.56s/it] 47%|████▋     | 44/93 [04:49<05:38,  6.90s/it] 48%|████▊     | 45/93 [04:55<05:13,  6.53s/it] 49%|████▉     | 46/93 [05:01<04:52,  6.23s/it] 51%|█████     | 47/93 [05:06<04:36,  6.01s/it] 52%|█████▏    | 48/93 [05:14<04:54,  6.55s/it] 53%|█████▎    | 49/93 [05:22<05:04,  6.92s/it] 54%|█████▍    | 50/93 [05:28<04:49,  6.72s/it]                                               {'loss': 1.8821, 'grad_norm': 0.8747382164001465, 'learning_rate': 5.180722891566266e-06, 'epoch': 1.58}
 54%|█████▍    | 50/93 [05:28<04:49,  6.72s/it] 55%|█████▍    | 51/93 [05:35<04:49,  6.88s/it] 56%|█████▌    | 52/93 [05:41<04:33,  6.68s/it] 57%|█████▋    | 53/93 [05:48<04:26,  6.66s/it] 58%|█████▊    | 54/93 [05:55<04:24,  6.78s/it] 59%|█████▉    | 55/93 [06:01<04:10,  6.60s/it] 60%|██████    | 56/93 [06:08<04:04,  6.61s/it] 61%|██████▏   | 57/93 [06:16<04:16,  7.12s/it] 62%|██████▏   | 58/93 [06:23<04:05,  7.02s/it] 63%|██████▎   | 59/93 [06:32<04:18,  7.62s/it] 65%|██████▍   | 60/93 [06:39<04:05,  7.43s/it]                                               {'loss': 1.6737, 'grad_norm': 0.9752487540245056, 'learning_rate': 3.975903614457832e-06, 'epoch': 1.9}
 65%|██████▍   | 60/93 [06:39<04:05,  7.43s/it] 66%|██████▌   | 61/93 [06:45<03:46,  7.09s/it] 67%|██████▋   | 62/93 [06:52<03:36,  7.00s/it] 68%|██████▊   | 63/93 [06:59<03:32,  7.07s/it] 69%|██████▉   | 64/93 [07:01<02:36,  5.40s/it] 70%|██████▉   | 65/93 [07:09<02:51,  6.14s/it] 71%|███████   | 66/93 [07:15<02:44,  6.10s/it] 72%|███████▏  | 67/93 [07:22<02:44,  6.35s/it] 73%|███████▎  | 68/93 [07:27<02:35,  6.22s/it] 74%|███████▍  | 69/93 [07:35<02:38,  6.61s/it] 75%|███████▌  | 70/93 [07:42<02:32,  6.62s/it]                                               {'loss': 1.7282, 'grad_norm': 0.8958094120025635, 'learning_rate': 2.771084337349398e-06, 'epoch': 2.19}
 75%|███████▌  | 70/93 [07:42<02:32,  6.62s/it] 76%|███████▋  | 71/93 [07:50<02:35,  7.08s/it] 77%|███████▋  | 72/93 [07:56<02:25,  6.92s/it] 78%|███████▊  | 73/93 [08:04<02:21,  7.06s/it] 80%|███████▉  | 74/93 [08:11<02:13,  7.04s/it] 81%|████████  | 75/93 [08:17<02:01,  6.74s/it] 82%|████████▏ | 76/93 [08:24<01:54,  6.76s/it] 83%|████████▎ | 77/93 [08:30<01:46,  6.65s/it] 84%|████████▍ | 78/93 [08:36<01:38,  6.55s/it] 85%|████████▍ | 79/93 [08:43<01:33,  6.67s/it] 86%|████████▌ | 80/93 [08:50<01:25,  6.60s/it]                                               {'loss': 1.5983, 'grad_norm': 1.2019144296646118, 'learning_rate': 1.566265060240964e-06, 'epoch': 2.51}
 86%|████████▌ | 80/93 [08:50<01:25,  6.60s/it] 87%|████████▋ | 81/93 [08:56<01:17,  6.46s/it] 88%|████████▊ | 82/93 [09:02<01:09,  6.33s/it] 89%|████████▉ | 83/93 [09:08<01:04,  6.41s/it] 90%|█████████ | 84/93 [09:14<00:56,  6.26s/it] 91%|█████████▏| 85/93 [09:20<00:48,  6.01s/it] 92%|█████████▏| 86/93 [09:27<00:44,  6.37s/it] 94%|█████████▎| 87/93 [09:33<00:38,  6.43s/it] 95%|█████████▍| 88/93 [09:40<00:32,  6.47s/it] 96%|█████████▌| 89/93 [09:45<00:24,  6.07s/it] 97%|█████████▋| 90/93 [09:51<00:18,  6.12s/it]                                               {'loss': 1.6293, 'grad_norm': 1.0437743663787842, 'learning_rate': 3.614457831325301e-07, 'epoch': 2.83}
 97%|█████████▋| 90/93 [09:51<00:18,  6.12s/it] 98%|█████████▊| 91/93 [09:57<00:12,  6.00s/it] 99%|█████████▉| 92/93 [10:07<00:07,  7.25s/it]100%|██████████| 93/93 [10:13<00:00,  6.89s/it][INFO|trainer.py:3942] 2025-04-13 18:13:24,862 >> Saving model checkpoint to saves/llama2-7b/lora/sft/checkpoint-93
[INFO|configuration_utils.py:697] 2025-04-13 18:13:24,889 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 18:13:24,890 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-04-13 18:13:25,019 >> tokenizer config file saved in saves/llama2-7b/lora/sft/checkpoint-93/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-04-13 18:13:25,020 >> Special tokens file saved in saves/llama2-7b/lora/sft/checkpoint-93/special_tokens_map.json
[INFO|trainer.py:2657] 2025-04-13 18:13:25,252 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 614.2804, 'train_samples_per_second': 4.884, 'train_steps_per_second': 0.151, 'train_loss': 1.77895995109312, 'epoch': 2.93}
100%|██████████| 93/93 [10:14<00:00,  6.89s/it]100%|██████████| 93/93 [10:14<00:00,  6.61s/it]
[INFO|trainer.py:3942] 2025-04-13 18:13:25,291 >> Saving model checkpoint to saves/llama2-7b/lora/sft
[INFO|configuration_utils.py:697] 2025-04-13 18:13:25,320 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 18:13:25,321 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-04-13 18:13:25,435 >> tokenizer config file saved in saves/llama2-7b/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-04-13 18:13:25,435 >> Special tokens file saved in saves/llama2-7b/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.928
  total_flos               = 60773397GF
  train_loss               =      1.779
  train_runtime            = 0:10:14.28
  train_samples_per_second =      4.884
  train_steps_per_second   =      0.151
Figure saved at: saves/llama2-7b/lora/sft/training_loss.png
[WARNING|2025-04-13 18:13:25] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-04-13 18:13:25] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:449] 2025-04-13 18:13:25,602 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
