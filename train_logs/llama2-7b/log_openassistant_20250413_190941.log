INFO 04-13 19:09:46 __init__.py:190] Automatically detected platform cuda.
[INFO|2025-04-13 19:09:48] llamafactory.cli:143 >> Initializing 1 distributed tasks at: 127.0.0.1:25837
[WARNING|2025-04-13 19:09:54] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-04-13 19:09:54] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,509 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,509 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,509 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,509 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,509 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,509 >> loading file chat_template.jinja
[INFO|configuration_utils.py:697] 2025-04-13 19:09:54,608 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 19:09:54,609 >> Model config LlamaConfig {
  "_name_or_path": "/data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,609 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,609 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,609 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,609 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,610 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-04-13 19:09:54,610 >> loading file chat_template.jinja
[INFO|2025-04-13 19:09:54] llamafactory.data.template:143 >> Add pad token: </s>
[INFO|2025-04-13 19:09:54] llamafactory.data.loader:143 >> Loading dataset openassistant.json...
Converting format of dataset (num_proc=16):   0%|          | 0/9846 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 616/9846 [00:00<00:01, 5393.96 examples/s]Converting format of dataset (num_proc=16):  75%|███████▌  | 7386/9846 [00:00<00:00, 39945.81 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 9846/9846 [00:00<00:00, 28977.44 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/9846 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 616/9846 [00:00<00:10, 871.42 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 2464/9846 [00:00<00:02, 3600.13 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 5541/9846 [00:00<00:00, 7913.29 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 7386/9846 [00:01<00:00, 9341.91 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 9846/9846 [00:01<00:00, 12375.46 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 9846/9846 [00:01<00:00, 7489.57 examples/s] 
training example:
input_ids:
[1, 518, 25580, 29962, 376, 7185, 459, 1100, 29891, 29908, 14637, 304, 263, 9999, 3829, 988, 727, 338, 871, 697, 1321, 7598, 363, 263, 3153, 1781, 470, 2669, 29889, 512, 7766, 1199, 29892, 445, 1840, 338, 10734, 8018, 297, 278, 10212, 9999, 29892, 988, 263, 1601, 459, 1100, 29891, 5703, 261, 756, 7282, 3081, 975, 278, 281, 1179, 322, 1985, 5855, 310, 1009, 22873, 29889, 450, 10122, 310, 263, 1601, 459, 1100, 29891, 508, 1121, 297, 5224, 281, 1179, 322, 12212, 5703, 358, 28602, 1907, 363, 17162, 29892, 408, 278, 5703, 261, 756, 2217, 297, 1760, 573, 304, 7910, 281, 1179, 470, 3867, 2253, 1985, 5855, 29889, 13, 13, 4789, 296, 5925, 756, 15659, 7037, 1601, 459, 1100, 583, 297, 6397, 2722, 1316, 408, 3240, 737, 322, 5172, 9687, 29892, 988, 263, 2846, 2919, 14582, 2761, 263, 7282, 11910, 310, 278, 9999, 313, 29933, 440, 575, 669, 341, 728, 295, 29892, 29871, 29906, 29900, 29896, 29941, 467, 512, 1438, 6397, 2722, 29892, 17162, 4049, 3700, 4482, 281, 1179, 29892, 9078, 23633, 29892, 322, 12212, 289, 1191, 17225, 3081, 29892, 8236, 304, 263, 6434, 988, 896, 526, 14278, 373, 278, 5703, 261, 363, 1009, 7294, 22342, 29889, 910, 26307, 508, 1121, 297, 4340, 1462, 23881, 310, 281, 1179, 322, 263, 4845, 457, 297, 1985, 5855, 29889, 13, 13, 3563, 497, 29892, 278, 6964, 310, 1601, 459, 1100, 29891, 338, 18853, 304, 8004, 278, 19753, 310, 10212, 2791, 1691, 322, 278, 10879, 310, 9999, 3081, 373, 17162, 29889, 8725, 5925, 338, 4312, 304, 2274, 278, 15834, 322, 10879, 310, 1601, 459, 1100, 583, 373, 278, 26504, 322, 304, 2693, 24833, 304, 3211, 445, 2228, 29889, 13, 13, 1123, 10662, 29901, 13, 29933, 440, 575, 29892, 435, 1696, 669, 341, 728, 295, 29892, 365, 29889, 313, 29906, 29900, 29896, 29941, 467, 450, 14617, 310, 12767, 403, 11080, 329, 3145, 322, 4231, 273, 1455, 6175, 1211, 1338, 408, 7298, 5084, 310, 390, 1237, 297, 7488, 29871, 29896, 2431, 1760, 512, 26807, 29889, 8237, 310, 12884, 293, 9034, 1103, 3145, 29892, 29871, 29906, 29955, 29898, 29941, 511, 29871, 29945, 29955, 29899, 29955, 29947, 29889, 518, 29914, 25580, 29962, 1815, 366, 2436, 263, 3273, 18707, 1048, 278, 29527, 749, 310, 278, 1840, 376, 3712, 459, 1100, 29891, 29908, 297, 7766, 1199, 29973, 3529, 671, 6455, 4475, 304, 7037, 1601, 459, 1100, 583, 297, 278, 23390, 9999, 322, 274, 568, 8018, 5925, 29889, 2]
inputs:
<s> [INST] "Monopsony" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.

Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.

Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.

References:
Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78. [/INST] Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1815, 366, 2436, 263, 3273, 18707, 1048, 278, 29527, 749, 310, 278, 1840, 376, 3712, 459, 1100, 29891, 29908, 297, 7766, 1199, 29973, 3529, 671, 6455, 4475, 304, 7037, 1601, 459, 1100, 583, 297, 278, 23390, 9999, 322, 274, 568, 8018, 5925, 29889, 2]
labels:
Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</s>
[INFO|configuration_utils.py:697] 2025-04-13 19:09:57,563 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 19:09:57,564 >> Model config LlamaConfig {
  "_name_or_path": "/data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3979] 2025-04-13 19:09:57,604 >> loading weights file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1633] 2025-04-13 19:09:57,604 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1140] 2025-04-13 19:09:57,606 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
[INFO|modeling_utils.py:4970] 2025-04-13 19:10:01,661 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4978] 2025-04-13 19:10:01,661 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1093] 2025-04-13 19:10:01,744 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:1140] 2025-04-13 19:10:01,744 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|2025-04-13 19:10:01] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-04-13 19:10:01] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-04-13 19:10:01] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-04-13 19:10:01] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-04-13 19:10:01] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,up_proj,gate_proj,down_proj,o_proj,q_proj,v_proj
[INFO|2025-04-13 19:10:02] llamafactory.model.loader:143 >> trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:746] 2025-04-13 19:10:02,304 >> Using auto half precision backend
[WARNING|trainer.py:781] 2025-04-13 19:10:02,305 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[INFO|trainer.py:2405] 2025-04-13 19:10:02,976 >> ***** Running training *****
[INFO|trainer.py:2406] 2025-04-13 19:10:02,976 >>   Num examples = 9,846
[INFO|trainer.py:2407] 2025-04-13 19:10:02,976 >>   Num Epochs = 3
[INFO|trainer.py:2408] 2025-04-13 19:10:02,976 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2411] 2025-04-13 19:10:02,976 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2412] 2025-04-13 19:10:02,976 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2413] 2025-04-13 19:10:02,976 >>   Total optimization steps = 921
[INFO|trainer.py:2414] 2025-04-13 19:10:02,981 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/921 [00:00<?, ?it/s]  0%|          | 1/921 [00:07<1:51:02,  7.24s/it]  0%|          | 2/921 [00:13<1:38:25,  6.43s/it]  0%|          | 3/921 [00:19<1:37:44,  6.39s/it]  0%|          | 4/921 [00:25<1:37:36,  6.39s/it]  1%|          | 5/921 [00:32<1:37:45,  6.40s/it]  1%|          | 6/921 [00:40<1:46:17,  6.97s/it]  1%|          | 7/921 [00:45<1:39:33,  6.54s/it]  1%|          | 8/921 [00:52<1:38:19,  6.46s/it]  1%|          | 9/921 [00:58<1:36:20,  6.34s/it]  1%|          | 10/921 [01:04<1:34:47,  6.24s/it]                                                  {'loss': 1.8388, 'grad_norm': 0.3727937340736389, 'learning_rate': 1.0752688172043011e-06, 'epoch': 0.03}
  1%|          | 10/921 [01:04<1:34:47,  6.24s/it]  1%|          | 11/921 [01:11<1:39:38,  6.57s/it]  1%|▏         | 12/921 [01:20<1:49:52,  7.25s/it]  1%|▏         | 13/921 [01:29<1:56:49,  7.72s/it]  2%|▏         | 14/921 [01:36<1:55:21,  7.63s/it]  2%|▏         | 15/921 [01:44<1:55:59,  7.68s/it]  2%|▏         | 16/921 [01:52<1:56:25,  7.72s/it]  2%|▏         | 17/921 [01:57<1:45:29,  7.00s/it]  2%|▏         | 18/921 [02:04<1:46:20,  7.07s/it]  2%|▏         | 19/921 [02:12<1:49:37,  7.29s/it]  2%|▏         | 20/921 [02:20<1:51:33,  7.43s/it]                                                  {'loss': 1.8675, 'grad_norm': 0.36079052090644836, 'learning_rate': 2.1505376344086023e-06, 'epoch': 0.06}
  2%|▏         | 20/921 [02:20<1:51:33,  7.43s/it]  2%|▏         | 21/921 [02:26<1:44:28,  6.97s/it]  2%|▏         | 22/921 [02:33<1:43:48,  6.93s/it]  2%|▏         | 23/921 [02:40<1:45:10,  7.03s/it]  3%|▎         | 24/921 [02:47<1:47:08,  7.17s/it]  3%|▎         | 25/921 [02:55<1:47:04,  7.17s/it]  3%|▎         | 26/921 [03:03<1:54:01,  7.64s/it]  3%|▎         | 27/921 [03:12<2:00:00,  8.05s/it]  3%|▎         | 28/921 [03:19<1:52:37,  7.57s/it]  3%|▎         | 29/921 [03:25<1:46:49,  7.19s/it]  3%|▎         | 30/921 [03:32<1:46:33,  7.18s/it]                                                  {'loss': 1.8535, 'grad_norm': 0.4764156937599182, 'learning_rate': 3.225806451612903e-06, 'epoch': 0.1}
  3%|▎         | 30/921 [03:32<1:46:33,  7.18s/it]  3%|▎         | 31/921 [03:42<1:57:44,  7.94s/it]  3%|▎         | 32/921 [03:50<1:59:14,  8.05s/it]  4%|▎         | 33/921 [03:58<1:55:59,  7.84s/it]  4%|▎         | 34/921 [04:04<1:50:56,  7.50s/it]  4%|▍         | 35/921 [04:12<1:51:12,  7.53s/it]  4%|▍         | 36/921 [04:19<1:49:32,  7.43s/it]  4%|▍         | 37/921 [04:26<1:46:02,  7.20s/it]  4%|▍         | 38/921 [04:33<1:45:23,  7.16s/it]  4%|▍         | 39/921 [04:39<1:39:56,  6.80s/it]  4%|▍         | 40/921 [04:45<1:37:53,  6.67s/it]                                                  {'loss': 1.9009, 'grad_norm': 0.5306467413902283, 'learning_rate': 4.3010752688172045e-06, 'epoch': 0.13}
  4%|▍         | 40/921 [04:45<1:37:53,  6.67s/it]  4%|▍         | 41/921 [04:52<1:40:22,  6.84s/it]  5%|▍         | 42/921 [04:59<1:40:05,  6.83s/it]  5%|▍         | 43/921 [05:07<1:44:30,  7.14s/it]  5%|▍         | 44/921 [05:13<1:37:52,  6.70s/it]  5%|▍         | 45/921 [05:20<1:39:30,  6.82s/it]  5%|▍         | 46/921 [05:26<1:37:33,  6.69s/it]  5%|▌         | 47/921 [05:33<1:37:20,  6.68s/it]  5%|▌         | 48/921 [05:39<1:36:31,  6.63s/it]  5%|▌         | 49/921 [05:46<1:35:18,  6.56s/it]  5%|▌         | 50/921 [05:57<1:54:19,  7.87s/it]                                                  {'loss': 1.8918, 'grad_norm': 0.5089547634124756, 'learning_rate': 5.376344086021506e-06, 'epoch': 0.16}
  5%|▌         | 50/921 [05:57<1:54:19,  7.87s/it]  6%|▌         | 51/921 [06:04<1:52:10,  7.74s/it]  6%|▌         | 52/921 [06:13<1:57:41,  8.13s/it]  6%|▌         | 53/921 [06:19<1:47:43,  7.45s/it]  6%|▌         | 54/921 [06:26<1:44:34,  7.24s/it]  6%|▌         | 55/921 [06:34<1:46:45,  7.40s/it]  6%|▌         | 56/921 [06:39<1:38:57,  6.86s/it]  6%|▌         | 57/921 [06:45<1:35:52,  6.66s/it]  6%|▋         | 58/921 [06:53<1:41:49,  7.08s/it]  6%|▋         | 59/921 [07:01<1:43:31,  7.21s/it]  7%|▋         | 60/921 [07:08<1:42:31,  7.15s/it]                                                  {'loss': 1.7482, 'grad_norm': 0.5307468771934509, 'learning_rate': 6.451612903225806e-06, 'epoch': 0.19}
  7%|▋         | 60/921 [07:08<1:42:31,  7.15s/it]  7%|▋         | 61/921 [07:16<1:45:38,  7.37s/it]  7%|▋         | 62/921 [07:21<1:36:01,  6.71s/it]  7%|▋         | 63/921 [07:28<1:36:26,  6.74s/it]  7%|▋         | 64/921 [07:34<1:34:58,  6.65s/it]  7%|▋         | 65/921 [07:39<1:26:47,  6.08s/it]  7%|▋         | 66/921 [07:46<1:30:24,  6.34s/it]  7%|▋         | 67/921 [07:54<1:37:38,  6.86s/it]  7%|▋         | 68/921 [07:59<1:30:03,  6.33s/it]  7%|▋         | 69/921 [08:06<1:32:02,  6.48s/it]  8%|▊         | 70/921 [08:12<1:28:47,  6.26s/it]                                                  {'loss': 1.7902, 'grad_norm': 0.5389223098754883, 'learning_rate': 7.526881720430108e-06, 'epoch': 0.23}
  8%|▊         | 70/921 [08:12<1:28:47,  6.26s/it]  8%|▊         | 71/921 [08:19<1:31:55,  6.49s/it]  8%|▊         | 72/921 [08:25<1:28:50,  6.28s/it]  8%|▊         | 73/921 [08:31<1:27:31,  6.19s/it]  8%|▊         | 74/921 [08:42<1:48:34,  7.69s/it]  8%|▊         | 75/921 [08:50<1:52:48,  8.00s/it]  8%|▊         | 76/921 [08:57<1:46:06,  7.53s/it]  8%|▊         | 77/921 [09:05<1:46:56,  7.60s/it]  8%|▊         | 78/921 [09:11<1:41:48,  7.25s/it]  9%|▊         | 79/921 [09:17<1:38:10,  7.00s/it]  9%|▊         | 80/921 [09:24<1:37:28,  6.95s/it]                                                  {'loss': 1.6614, 'grad_norm': 0.7529381513595581, 'learning_rate': 8.602150537634409e-06, 'epoch': 0.26}
  9%|▊         | 80/921 [09:24<1:37:28,  6.95s/it]  9%|▉         | 81/921 [09:32<1:39:29,  7.11s/it]  9%|▉         | 82/921 [09:39<1:39:32,  7.12s/it]  9%|▉         | 83/921 [09:49<1:50:02,  7.88s/it]  9%|▉         | 84/921 [09:54<1:38:33,  7.07s/it]  9%|▉         | 85/921 [09:59<1:29:46,  6.44s/it]  9%|▉         | 86/921 [10:04<1:26:36,  6.22s/it]  9%|▉         | 87/921 [10:13<1:35:17,  6.86s/it] 10%|▉         | 88/921 [10:19<1:34:22,  6.80s/it] 10%|▉         | 89/921 [10:26<1:34:03,  6.78s/it] 10%|▉         | 90/921 [10:33<1:33:14,  6.73s/it]                                                  {'loss': 1.5823, 'grad_norm': 0.7868953347206116, 'learning_rate': 9.67741935483871e-06, 'epoch': 0.29}
 10%|▉         | 90/921 [10:33<1:33:14,  6.73s/it] 10%|▉         | 91/921 [10:40<1:33:52,  6.79s/it] 10%|▉         | 92/921 [10:46<1:32:17,  6.68s/it] 10%|█         | 93/921 [10:55<1:39:50,  7.23s/it] 10%|█         | 94/921 [11:00<1:33:39,  6.80s/it] 10%|█         | 95/921 [11:07<1:31:35,  6.65s/it] 10%|█         | 96/921 [11:15<1:36:42,  7.03s/it] 11%|█         | 97/921 [11:22<1:37:22,  7.09s/it] 11%|█         | 98/921 [11:30<1:40:41,  7.34s/it] 11%|█         | 99/921 [11:38<1:46:00,  7.74s/it] 11%|█         | 100/921 [11:46<1:43:45,  7.58s/it]                                                   {'loss': 1.5529, 'grad_norm': 0.7858144044876099, 'learning_rate': 9.998236604894158e-06, 'epoch': 0.32}
 11%|█         | 100/921 [11:46<1:43:45,  7.58s/it] 11%|█         | 101/921 [11:53<1:41:40,  7.44s/it] 11%|█         | 102/921 [11:59<1:36:26,  7.07s/it] 11%|█         | 103/921 [12:06<1:37:58,  7.19s/it] 11%|█▏        | 104/921 [12:13<1:35:52,  7.04s/it] 11%|█▏        | 105/921 [12:19<1:30:31,  6.66s/it] 12%|█▏        | 106/921 [12:26<1:33:39,  6.89s/it] 12%|█▏        | 107/921 [12:32<1:30:13,  6.65s/it] 12%|█▏        | 108/921 [12:39<1:27:35,  6.46s/it] 12%|█▏        | 109/921 [12:46<1:32:10,  6.81s/it] 12%|█▏        | 110/921 [12:54<1:35:34,  7.07s/it]                                                   {'loss': 1.4377, 'grad_norm': 0.7135145664215088, 'learning_rate': 9.989602561804816e-06, 'epoch': 0.36}
 12%|█▏        | 110/921 [12:54<1:35:34,  7.07s/it] 12%|█▏        | 111/921 [13:00<1:33:19,  6.91s/it] 12%|█▏        | 112/921 [13:07<1:31:21,  6.78s/it] 12%|█▏        | 113/921 [13:14<1:32:51,  6.90s/it] 12%|█▏        | 114/921 [13:20<1:30:08,  6.70s/it] 12%|█▏        | 115/921 [13:29<1:36:57,  7.22s/it] 13%|█▎        | 116/921 [13:37<1:41:42,  7.58s/it] 13%|█▎        | 117/921 [13:45<1:41:14,  7.55s/it] 13%|█▎        | 118/921 [13:52<1:41:19,  7.57s/it] 13%|█▎        | 119/921 [13:59<1:37:03,  7.26s/it] 13%|█▎        | 120/921 [14:06<1:35:53,  7.18s/it]                                                   {'loss': 1.4838, 'grad_norm': 0.7875105738639832, 'learning_rate': 9.973786394290475e-06, 'epoch': 0.39}
 13%|█▎        | 120/921 [14:06<1:35:53,  7.18s/it] 13%|█▎        | 121/921 [14:12<1:30:37,  6.80s/it] 13%|█▎        | 122/921 [14:18<1:29:01,  6.69s/it] 13%|█▎        | 123/921 [14:26<1:32:44,  6.97s/it] 13%|█▎        | 124/921 [14:32<1:32:00,  6.93s/it] 14%|█▎        | 125/921 [14:39<1:30:44,  6.84s/it] 14%|█▎        | 126/921 [14:47<1:35:33,  7.21s/it] 14%|█▍        | 127/921 [14:56<1:42:31,  7.75s/it] 14%|█▍        | 128/921 [15:03<1:39:03,  7.50s/it] 14%|█▍        | 129/921 [15:12<1:42:32,  7.77s/it] 14%|█▍        | 130/921 [15:19<1:42:55,  7.81s/it]                                                   {'loss': 1.4575, 'grad_norm': 0.6354480385780334, 'learning_rate': 9.950810868429515e-06, 'epoch': 0.42}
 14%|█▍        | 130/921 [15:19<1:42:55,  7.81s/it] 14%|█▍        | 131/921 [15:26<1:38:04,  7.45s/it] 14%|█▍        | 132/921 [15:33<1:35:12,  7.24s/it] 14%|█▍        | 133/921 [15:41<1:39:28,  7.57s/it] 15%|█▍        | 134/921 [15:48<1:35:30,  7.28s/it] 15%|█▍        | 135/921 [15:53<1:28:19,  6.74s/it] 15%|█▍        | 136/921 [16:00<1:29:35,  6.85s/it] 15%|█▍        | 137/921 [16:09<1:36:48,  7.41s/it] 15%|█▍        | 138/921 [16:18<1:44:12,  7.99s/it] 15%|█▌        | 139/921 [16:25<1:36:51,  7.43s/it] 15%|█▌        | 140/921 [16:31<1:33:43,  7.20s/it]                                                   {'loss': 1.4357, 'grad_norm': 0.5379547476768494, 'learning_rate': 9.920709055610572e-06, 'epoch': 0.45}
 15%|█▌        | 140/921 [16:31<1:33:43,  7.20s/it] 15%|█▌        | 141/921 [16:38<1:34:00,  7.23s/it] 15%|█▌        | 142/921 [16:44<1:28:15,  6.80s/it] 16%|█▌        | 143/921 [16:51<1:26:45,  6.69s/it] 16%|█▌        | 144/921 [16:58<1:30:53,  7.02s/it] 16%|█▌        | 145/921 [17:04<1:24:09,  6.51s/it] 16%|█▌        | 146/921 [17:10<1:23:58,  6.50s/it] 16%|█▌        | 147/921 [17:17<1:23:53,  6.50s/it] 16%|█▌        | 148/921 [17:24<1:25:17,  6.62s/it] 16%|█▌        | 149/921 [17:30<1:22:40,  6.43s/it] 16%|█▋        | 150/921 [17:36<1:20:49,  6.29s/it]                                                   {'loss': 1.4162, 'grad_norm': 0.9057642221450806, 'learning_rate': 9.883524284928982e-06, 'epoch': 0.49}
 16%|█▋        | 150/921 [17:36<1:20:49,  6.29s/it] 16%|█▋        | 151/921 [17:44<1:27:56,  6.85s/it] 17%|█▋        | 152/921 [17:51<1:29:40,  7.00s/it] 17%|█▋        | 153/921 [17:58<1:28:45,  6.93s/it] 17%|█▋        | 154/921 [18:04<1:24:11,  6.59s/it] 17%|█▋        | 155/921 [18:11<1:26:21,  6.76s/it] 17%|█▋        | 156/921 [18:17<1:25:19,  6.69s/it] 17%|█▋        | 157/921 [18:25<1:28:45,  6.97s/it] 17%|█▋        | 158/921 [18:35<1:40:34,  7.91s/it] 17%|█▋        | 159/921 [18:42<1:36:37,  7.61s/it] 17%|█▋        | 160/921 [18:50<1:37:26,  7.68s/it]                                                   {'loss': 1.3273, 'grad_norm': 0.4160988926887512, 'learning_rate': 9.839310080818092e-06, 'epoch': 0.52}
 17%|█▋        | 160/921 [18:50<1:37:26,  7.68s/it] 17%|█▋        | 161/921 [18:56<1:30:03,  7.11s/it] 18%|█▊        | 162/921 [19:06<1:42:13,  8.08s/it] 18%|█▊        | 163/921 [19:11<1:31:37,  7.25s/it] 18%|█▊        | 164/921 [19:18<1:30:09,  7.15s/it] 18%|█▊        | 165/921 [19:27<1:34:41,  7.51s/it] 18%|█▊        | 166/921 [19:32<1:28:10,  7.01s/it] 18%|█▊        | 167/921 [19:43<1:41:05,  8.04s/it] 18%|█▊        | 168/921 [19:51<1:40:11,  7.98s/it] 18%|█▊        | 169/921 [19:58<1:38:32,  7.86s/it] 18%|█▊        | 170/921 [20:04<1:31:07,  7.28s/it]                                                   {'loss': 1.314, 'grad_norm': 0.7882722616195679, 'learning_rate': 9.788130086005222e-06, 'epoch': 0.55}
 18%|█▊        | 170/921 [20:04<1:31:07,  7.28s/it] 19%|█▊        | 171/921 [20:12<1:34:01,  7.52s/it] 19%|█▊        | 172/921 [20:19<1:30:20,  7.24s/it] 19%|█▉        | 173/921 [20:28<1:38:48,  7.93s/it] 19%|█▉        | 174/921 [20:38<1:45:11,  8.45s/it] 19%|█▉        | 175/921 [20:44<1:34:11,  7.58s/it] 19%|█▉        | 176/921 [20:51<1:33:49,  7.56s/it] 19%|█▉        | 177/921 [20:58<1:30:51,  7.33s/it] 19%|█▉        | 178/921 [21:04<1:27:34,  7.07s/it] 19%|█▉        | 179/921 [21:13<1:31:20,  7.39s/it] 20%|█▉        | 180/921 [21:21<1:35:55,  7.77s/it]                                                   {'loss': 1.3319, 'grad_norm': 0.46043193340301514, 'learning_rate': 9.73005796990317e-06, 'epoch': 0.58}
 20%|█▉        | 180/921 [21:21<1:35:55,  7.77s/it] 20%|█▉        | 181/921 [21:30<1:38:38,  8.00s/it] 20%|█▉        | 182/921 [21:35<1:27:57,  7.14s/it] 20%|█▉        | 183/921 [21:42<1:26:57,  7.07s/it] 20%|█▉        | 184/921 [21:50<1:32:47,  7.55s/it] 20%|██        | 185/921 [21:56<1:26:48,  7.08s/it] 20%|██        | 186/921 [22:05<1:31:10,  7.44s/it] 20%|██        | 187/921 [22:15<1:42:17,  8.36s/it] 20%|██        | 188/921 [22:21<1:33:57,  7.69s/it] 21%|██        | 189/921 [22:29<1:34:32,  7.75s/it] 21%|██        | 190/921 [22:36<1:31:54,  7.54s/it]                                                   {'loss': 1.3501, 'grad_norm': 0.8172356486320496, 'learning_rate': 9.665177322569117e-06, 'epoch': 0.62}
 21%|██        | 190/921 [22:36<1:31:54,  7.54s/it] 21%|██        | 191/921 [22:43<1:29:04,  7.32s/it] 21%|██        | 192/921 [22:51<1:30:43,  7.47s/it] 21%|██        | 193/921 [22:57<1:27:09,  7.18s/it] 21%|██        | 194/921 [23:05<1:28:16,  7.29s/it] 21%|██        | 195/921 [23:11<1:25:11,  7.04s/it] 21%|██▏       | 196/921 [23:17<1:19:41,  6.59s/it] 21%|██▏       | 197/921 [23:23<1:17:24,  6.41s/it] 21%|██▏       | 198/921 [23:32<1:25:49,  7.12s/it] 22%|██▏       | 199/921 [23:40<1:31:28,  7.60s/it] 22%|██▏       | 200/921 [23:48<1:30:09,  7.50s/it]                                                   {'loss': 1.5191, 'grad_norm': 0.6780253052711487, 'learning_rate': 9.593581534383595e-06, 'epoch': 0.65}
 22%|██▏       | 200/921 [23:48<1:30:09,  7.50s/it] 22%|██▏       | 201/921 [23:54<1:24:17,  7.02s/it] 22%|██▏       | 202/921 [24:00<1:22:23,  6.88s/it] 22%|██▏       | 203/921 [24:08<1:24:08,  7.03s/it] 22%|██▏       | 204/921 [24:15<1:25:10,  7.13s/it] 22%|██▏       | 205/921 [24:21<1:22:05,  6.88s/it] 22%|██▏       | 206/921 [24:28<1:22:04,  6.89s/it] 22%|██▏       | 207/921 [24:35<1:20:57,  6.80s/it] 23%|██▎       | 208/921 [24:41<1:17:58,  6.56s/it] 23%|██▎       | 209/921 [24:51<1:29:41,  7.56s/it] 23%|██▎       | 210/921 [24:56<1:22:35,  6.97s/it]                                                   {'loss': 1.4689, 'grad_norm': 0.540226936340332, 'learning_rate': 9.515373661622665e-06, 'epoch': 0.68}
 23%|██▎       | 210/921 [24:56<1:22:35,  6.97s/it] 23%|██▎       | 211/921 [25:06<1:34:11,  7.96s/it] 23%|██▎       | 212/921 [25:13<1:28:45,  7.51s/it] 23%|██▎       | 213/921 [25:20<1:28:46,  7.52s/it] 23%|██▎       | 214/921 [25:29<1:30:51,  7.71s/it] 23%|██▎       | 215/921 [25:37<1:31:50,  7.80s/it] 23%|██▎       | 216/921 [25:43<1:27:18,  7.43s/it] 24%|██▎       | 217/921 [25:51<1:29:34,  7.63s/it] 24%|██▎       | 218/921 [25:59<1:30:59,  7.77s/it] 24%|██▍       | 219/921 [26:05<1:23:11,  7.11s/it] 24%|██▍       | 220/921 [26:12<1:21:14,  6.95s/it]                                                   {'loss': 1.3122, 'grad_norm': 0.5408902764320374, 'learning_rate': 9.43066627811685e-06, 'epoch': 0.71}
 24%|██▍       | 220/921 [26:12<1:21:14,  6.95s/it] 24%|██▍       | 221/921 [26:20<1:25:38,  7.34s/it] 24%|██▍       | 222/921 [26:27<1:25:28,  7.34s/it] 24%|██▍       | 223/921 [26:35<1:27:56,  7.56s/it] 24%|██▍       | 224/921 [26:46<1:39:43,  8.58s/it] 24%|██▍       | 225/921 [26:53<1:33:30,  8.06s/it] 25%|██▍       | 226/921 [27:00<1:29:17,  7.71s/it] 25%|██▍       | 227/921 [27:07<1:26:31,  7.48s/it] 25%|██▍       | 228/921 [27:16<1:30:36,  7.85s/it] 25%|██▍       | 229/921 [27:22<1:26:40,  7.51s/it] 25%|██▍       | 230/921 [27:28<1:21:01,  7.04s/it]                                                   {'loss': 1.415, 'grad_norm': 0.540143609046936, 'learning_rate': 9.339581313210332e-06, 'epoch': 0.75}
 25%|██▍       | 230/921 [27:28<1:21:01,  7.04s/it] 25%|██▌       | 231/921 [27:36<1:22:39,  7.19s/it] 25%|██▌       | 232/921 [27:43<1:22:17,  7.17s/it] 25%|██▌       | 233/921 [27:51<1:25:03,  7.42s/it] 25%|██▌       | 234/921 [27:56<1:17:39,  6.78s/it] 26%|██▌       | 235/921 [28:02<1:12:33,  6.35s/it] 26%|██▌       | 236/921 [28:08<1:14:35,  6.53s/it] 26%|██▌       | 237/921 [28:16<1:17:44,  6.82s/it] 26%|██▌       | 238/921 [28:21<1:12:25,  6.36s/it] 26%|██▌       | 239/921 [28:28<1:13:10,  6.44s/it] 26%|██▌       | 240/921 [28:34<1:11:43,  6.32s/it]                                                   {'loss': 1.3742, 'grad_norm': 0.7261133790016174, 'learning_rate': 9.242249876253617e-06, 'epoch': 0.78}
 26%|██▌       | 240/921 [28:34<1:11:43,  6.32s/it] 26%|██▌       | 241/921 [28:40<1:11:24,  6.30s/it] 26%|██▋       | 242/921 [28:47<1:14:00,  6.54s/it] 26%|██▋       | 243/921 [28:52<1:09:24,  6.14s/it] 26%|██▋       | 244/921 [28:59<1:10:54,  6.28s/it] 27%|██▋       | 245/921 [29:07<1:15:04,  6.66s/it] 27%|██▋       | 246/921 [29:14<1:16:05,  6.76s/it] 27%|██▋       | 247/921 [29:21<1:17:53,  6.93s/it] 27%|██▋       | 248/921 [29:29<1:20:23,  7.17s/it] 27%|██▋       | 249/921 [29:36<1:21:16,  7.26s/it] 27%|██▋       | 250/921 [29:42<1:17:03,  6.89s/it]                                                   {'loss': 1.5321, 'grad_norm': 0.8408838510513306, 'learning_rate': 9.138812067882388e-06, 'epoch': 0.81}
 27%|██▋       | 250/921 [29:42<1:17:03,  6.89s/it] 27%|██▋       | 251/921 [29:50<1:19:13,  7.09s/it] 27%|██▋       | 252/921 [29:56<1:16:00,  6.82s/it] 27%|██▋       | 253/921 [30:05<1:23:32,  7.50s/it] 28%|██▊       | 254/921 [30:11<1:18:10,  7.03s/it] 28%|██▊       | 255/921 [30:18<1:16:27,  6.89s/it] 28%|██▊       | 256/921 [30:26<1:21:24,  7.35s/it] 28%|██▊       | 257/921 [30:33<1:21:06,  7.33s/it] 28%|██▊       | 258/921 [30:41<1:20:59,  7.33s/it] 28%|██▊       | 259/921 [30:49<1:23:36,  7.58s/it] 28%|██▊       | 260/921 [30:57<1:24:13,  7.64s/it]                                                   {'loss': 1.3794, 'grad_norm': 0.9917583465576172, 'learning_rate': 9.02941677835409e-06, 'epoch': 0.84}
 28%|██▊       | 260/921 [30:57<1:24:13,  7.64s/it] 28%|██▊       | 261/921 [31:03<1:18:38,  7.15s/it] 28%|██▊       | 262/921 [31:09<1:16:13,  6.94s/it] 29%|██▊       | 263/921 [31:15<1:14:18,  6.78s/it] 29%|██▊       | 264/921 [31:21<1:10:30,  6.44s/it] 29%|██▉       | 265/921 [31:30<1:20:02,  7.32s/it] 29%|██▉       | 266/921 [31:36<1:14:33,  6.83s/it] 29%|██▉       | 267/921 [31:42<1:10:50,  6.50s/it] 29%|██▉       | 268/921 [31:49<1:14:11,  6.82s/it] 29%|██▉       | 269/921 [31:56<1:14:58,  6.90s/it] 29%|██▉       | 270/921 [32:03<1:13:58,  6.82s/it]                                                   {'loss': 1.3997, 'grad_norm': 0.55931556224823, 'learning_rate': 8.914221473232594e-06, 'epoch': 0.88}
 29%|██▉       | 270/921 [32:03<1:13:58,  6.82s/it] 29%|██▉       | 271/921 [32:10<1:12:51,  6.73s/it] 30%|██▉       | 272/921 [32:16<1:12:16,  6.68s/it] 30%|██▉       | 273/921 [32:24<1:15:09,  6.96s/it] 30%|██▉       | 274/921 [32:29<1:10:58,  6.58s/it] 30%|██▉       | 275/921 [32:36<1:11:23,  6.63s/it] 30%|██▉       | 276/921 [32:45<1:18:30,  7.30s/it] 30%|███       | 277/921 [32:51<1:15:27,  7.03s/it] 30%|███       | 278/921 [32:58<1:14:36,  6.96s/it] 30%|███       | 279/921 [33:07<1:19:36,  7.44s/it] 30%|███       | 280/921 [33:15<1:20:57,  7.58s/it]                                                   {'loss': 1.3621, 'grad_norm': 0.4757831394672394, 'learning_rate': 8.793391966729416e-06, 'epoch': 0.91}
 30%|███       | 280/921 [33:15<1:20:57,  7.58s/it] 31%|███       | 281/921 [33:21<1:16:59,  7.22s/it] 31%|███       | 282/921 [33:27<1:13:57,  6.94s/it] 31%|███       | 283/921 [33:35<1:15:12,  7.07s/it] 31%|███       | 284/921 [33:41<1:13:32,  6.93s/it] 31%|███       | 285/921 [33:47<1:09:50,  6.59s/it] 31%|███       | 286/921 [33:54<1:11:42,  6.78s/it] 31%|███       | 287/921 [34:04<1:21:11,  7.68s/it] 31%|███▏      | 288/921 [34:11<1:16:56,  7.29s/it] 31%|███▏      | 289/921 [34:17<1:15:03,  7.13s/it] 31%|███▏      | 290/921 [34:27<1:23:27,  7.94s/it]                                                   {'loss': 1.3385, 'grad_norm': 0.5632297992706299, 'learning_rate': 8.667102183027729e-06, 'epoch': 0.94}
 31%|███▏      | 290/921 [34:27<1:23:27,  7.94s/it] 32%|███▏      | 291/921 [34:34<1:21:16,  7.74s/it] 32%|███▏      | 292/921 [34:41<1:17:34,  7.40s/it] 32%|███▏      | 293/921 [34:48<1:15:13,  7.19s/it] 32%|███▏      | 294/921 [34:56<1:18:31,  7.51s/it] 32%|███▏      | 295/921 [35:05<1:22:55,  7.95s/it] 32%|███▏      | 296/921 [35:11<1:16:07,  7.31s/it] 32%|███▏      | 297/921 [35:18<1:15:28,  7.26s/it] 32%|███▏      | 298/921 [35:24<1:13:08,  7.04s/it] 32%|███▏      | 299/921 [35:32<1:15:17,  7.26s/it] 33%|███▎      | 300/921 [35:38<1:11:49,  6.94s/it]                                                   {'loss': 1.4001, 'grad_norm': 0.6940814852714539, 'learning_rate': 8.535533905932739e-06, 'epoch': 0.97}
 33%|███▎      | 300/921 [35:38<1:11:49,  6.94s/it] 33%|███▎      | 301/921 [35:46<1:12:50,  7.05s/it] 33%|███▎      | 302/921 [35:52<1:09:51,  6.77s/it] 33%|███▎      | 303/921 [35:58<1:08:38,  6.66s/it] 33%|███▎      | 304/921 [36:06<1:10:33,  6.86s/it] 33%|███▎      | 305/921 [36:19<1:29:28,  8.72s/it] 33%|███▎      | 306/921 [36:26<1:24:44,  8.27s/it] 33%|███▎      | 307/921 [36:32<1:16:49,  7.51s/it] 33%|███▎      | 308/921 [36:38<1:14:41,  7.31s/it] 34%|███▎      | 309/921 [36:47<1:18:59,  7.74s/it] 34%|███▎      | 310/921 [36:55<1:17:45,  7.64s/it]                                                   {'loss': 1.3419, 'grad_norm': 0.7547898888587952, 'learning_rate': 8.398876517208778e-06, 'epoch': 1.01}
 34%|███▎      | 310/921 [36:55<1:17:45,  7.64s/it] 34%|███▍      | 311/921 [37:02<1:17:02,  7.58s/it] 34%|███▍      | 312/921 [37:11<1:20:38,  7.95s/it] 34%|███▍      | 313/921 [37:18<1:18:46,  7.77s/it] 34%|███▍      | 314/921 [37:24<1:13:18,  7.25s/it] 34%|███▍      | 315/921 [37:32<1:13:44,  7.30s/it] 34%|███▍      | 316/921 [37:38<1:12:05,  7.15s/it] 34%|███▍      | 317/921 [37:47<1:15:56,  7.54s/it] 35%|███▍      | 318/921 [37:54<1:15:46,  7.54s/it] 35%|███▍      | 319/921 [38:03<1:18:15,  7.80s/it] 35%|███▍      | 320/921 [38:09<1:13:56,  7.38s/it]                                                   {'loss': 1.2759, 'grad_norm': 0.7101485729217529, 'learning_rate': 8.257326723979764e-06, 'epoch': 1.04}
 35%|███▍      | 320/921 [38:09<1:13:56,  7.38s/it] 35%|███▍      | 321/921 [38:16<1:10:38,  7.06s/it] 35%|███▍      | 322/921 [38:23<1:10:40,  7.08s/it] 35%|███▌      | 323/921 [38:29<1:08:55,  6.92s/it] 35%|███▌      | 324/921 [38:35<1:06:03,  6.64s/it] 35%|███▌      | 325/921 [38:41<1:03:58,  6.44s/it] 35%|███▌      | 326/921 [38:49<1:07:22,  6.79s/it] 36%|███▌      | 327/921 [38:55<1:04:06,  6.48s/it] 36%|███▌      | 328/921 [39:05<1:15:09,  7.60s/it] 36%|███▌      | 329/921 [39:12<1:12:40,  7.37s/it] 36%|███▌      | 330/921 [39:18<1:09:51,  7.09s/it]                                                   {'loss': 1.4259, 'grad_norm': 0.7626074552536011, 'learning_rate': 8.11108827558539e-06, 'epoch': 1.07}
 36%|███▌      | 330/921 [39:18<1:09:51,  7.09s/it] 36%|███▌      | 331/921 [39:24<1:06:39,  6.78s/it] 36%|███▌      | 332/921 [39:33<1:11:50,  7.32s/it] 36%|███▌      | 333/921 [39:40<1:13:01,  7.45s/it] 36%|███▋      | 334/921 [39:46<1:07:27,  6.89s/it] 36%|███▋      | 335/921 [39:53<1:08:06,  6.97s/it] 36%|███▋      | 336/921 [40:02<1:14:36,  7.65s/it] 37%|███▋      | 337/921 [40:08<1:09:07,  7.10s/it] 37%|███▋      | 338/921 [40:16<1:12:02,  7.42s/it] 37%|███▋      | 339/921 [40:23<1:10:34,  7.28s/it] 37%|███▋      | 340/921 [40:29<1:05:08,  6.73s/it]                                                   {'loss': 1.3915, 'grad_norm': 0.9017062783241272, 'learning_rate': 7.960371670300632e-06, 'epoch': 1.1}
 37%|███▋      | 340/921 [40:29<1:05:08,  6.73s/it] 37%|███▋      | 341/921 [40:35<1:03:55,  6.61s/it] 37%|███▋      | 342/921 [40:43<1:08:29,  7.10s/it] 37%|███▋      | 343/921 [40:51<1:11:20,  7.41s/it] 37%|███▋      | 344/921 [40:59<1:11:03,  7.39s/it] 37%|███▋      | 345/921 [41:05<1:07:46,  7.06s/it] 38%|███▊      | 346/921 [41:10<1:02:18,  6.50s/it] 38%|███▊      | 347/921 [41:20<1:09:58,  7.31s/it] 38%|███▊      | 348/921 [41:27<1:10:11,  7.35s/it] 38%|███▊      | 349/921 [41:35<1:12:21,  7.59s/it] 38%|███▊      | 350/921 [41:43<1:13:26,  7.72s/it]                                                   {'loss': 1.3762, 'grad_norm': 0.6400861144065857, 'learning_rate': 7.80539385234072e-06, 'epoch': 1.14}
 38%|███▊      | 350/921 [41:43<1:13:26,  7.72s/it] 38%|███▊      | 351/921 [41:49<1:08:21,  7.20s/it] 38%|███▊      | 352/921 [41:56<1:08:24,  7.21s/it] 38%|███▊      | 353/921 [42:03<1:07:25,  7.12s/it] 38%|███▊      | 354/921 [42:10<1:05:06,  6.89s/it] 39%|███▊      | 355/921 [42:19<1:12:11,  7.65s/it] 39%|███▊      | 356/921 [42:25<1:08:11,  7.24s/it] 39%|███▉      | 357/921 [42:34<1:10:42,  7.52s/it] 39%|███▉      | 358/921 [42:41<1:11:50,  7.66s/it] 39%|███▉      | 359/921 [42:48<1:07:42,  7.23s/it] 39%|███▉      | 360/921 [42:55<1:07:25,  7.21s/it]                                                   {'loss': 1.3983, 'grad_norm': 0.5199536681175232, 'learning_rate': 7.646377899587695e-06, 'epoch': 1.17}
 39%|███▉      | 360/921 [42:55<1:07:25,  7.21s/it] 39%|███▉      | 361/921 [43:01<1:03:18,  6.78s/it] 39%|███▉      | 362/921 [43:07<1:02:24,  6.70s/it] 39%|███▉      | 363/921 [43:15<1:05:11,  7.01s/it] 40%|███▉      | 364/921 [43:20<1:00:37,  6.53s/it] 40%|███▉      | 365/921 [43:32<1:15:40,  8.17s/it] 40%|███▉      | 366/921 [43:38<1:08:14,  7.38s/it] 40%|███▉      | 367/921 [43:46<1:11:12,  7.71s/it] 40%|███▉      | 368/921 [43:53<1:08:59,  7.48s/it] 40%|████      | 369/921 [44:00<1:06:26,  7.22s/it] 40%|████      | 370/921 [44:09<1:10:44,  7.70s/it]                                                   {'loss': 1.2783, 'grad_norm': 0.8026895523071289, 'learning_rate': 7.483552702488054e-06, 'epoch': 1.2}
 40%|████      | 370/921 [44:09<1:10:44,  7.70s/it] 40%|████      | 371/921 [44:16<1:08:36,  7.48s/it] 40%|████      | 372/921 [44:23<1:07:17,  7.35s/it] 40%|████      | 373/921 [44:28<1:01:15,  6.71s/it] 41%|████      | 374/921 [44:34<59:56,  6.58s/it]   41%|████      | 375/921 [44:41<1:01:11,  6.72s/it] 41%|████      | 376/921 [44:46<56:25,  6.21s/it]   41%|████      | 377/921 [44:54<59:07,  6.52s/it] 41%|████      | 378/921 [45:02<1:05:23,  7.23s/it] 41%|████      | 379/921 [45:08<1:01:53,  6.85s/it] 41%|████▏     | 380/921 [45:14<58:12,  6.46s/it]                                                   {'loss': 1.3708, 'grad_norm': 0.823421061038971, 'learning_rate': 7.3171526345837e-06, 'epoch': 1.23}
 41%|████▏     | 380/921 [45:14<58:12,  6.46s/it] 41%|████▏     | 381/921 [45:21<59:33,  6.62s/it] 41%|████▏     | 382/921 [45:28<1:00:56,  6.78s/it] 42%|████▏     | 383/921 [45:36<1:04:57,  7.24s/it] 42%|████▏     | 384/921 [45:43<1:03:49,  7.13s/it] 42%|████▏     | 385/921 [45:49<1:00:27,  6.77s/it] 42%|████▏     | 386/921 [45:57<1:02:10,  6.97s/it] 42%|████▏     | 387/921 [46:05<1:06:10,  7.43s/it] 42%|████▏     | 388/921 [46:11<1:00:52,  6.85s/it] 42%|████▏     | 389/921 [46:17<59:01,  6.66s/it]   42%|████▏     | 390/921 [46:25<1:03:34,  7.18s/it]                                                   {'loss': 1.3153, 'grad_norm': 0.5159453749656677, 'learning_rate': 7.147417215150411e-06, 'epoch': 1.27}
 42%|████▏     | 390/921 [46:25<1:03:34,  7.18s/it] 42%|████▏     | 391/921 [46:33<1:05:52,  7.46s/it] 43%|████▎     | 392/921 [46:41<1:07:27,  7.65s/it] 43%|████▎     | 393/921 [46:49<1:06:05,  7.51s/it] 43%|████▎     | 394/921 [46:56<1:05:16,  7.43s/it] 43%|████▎     | 395/921 [47:02<1:02:05,  7.08s/it] 43%|████▎     | 396/921 [47:08<59:18,  6.78s/it]   43%|████▎     | 397/921 [47:17<1:05:23,  7.49s/it] 43%|████▎     | 398/921 [47:25<1:04:49,  7.44s/it] 43%|████▎     | 399/921 [47:31<1:01:57,  7.12s/it] 43%|████▎     | 400/921 [47:38<1:01:21,  7.07s/it]                                                   {'loss': 1.3288, 'grad_norm': 0.6611207723617554, 'learning_rate': 6.974590764429447e-06, 'epoch': 1.3}
 43%|████▎     | 400/921 [47:38<1:01:21,  7.07s/it] 44%|████▎     | 401/921 [47:46<1:03:07,  7.28s/it] 44%|████▎     | 402/921 [47:53<1:04:03,  7.41s/it] 44%|████▍     | 403/921 [48:02<1:05:44,  7.62s/it] 44%|████▍     | 404/921 [48:07<1:00:41,  7.04s/it] 44%|████▍     | 405/921 [48:14<59:26,  6.91s/it]   44%|████▍     | 406/921 [48:22<1:03:37,  7.41s/it] 44%|████▍     | 407/921 [48:30<1:04:31,  7.53s/it] 44%|████▍     | 408/921 [48:37<1:01:39,  7.21s/it] 44%|████▍     | 409/921 [48:45<1:04:28,  7.56s/it] 45%|████▍     | 410/921 [48:51<58:46,  6.90s/it]                                                   {'loss': 1.3986, 'grad_norm': 0.6777687072753906, 'learning_rate': 6.798922051948569e-06, 'epoch': 1.33}
 45%|████▍     | 410/921 [48:51<58:46,  6.90s/it] 45%|████▍     | 411/921 [48:58<1:00:42,  7.14s/it] 45%|████▍     | 412/921 [49:04<56:01,  6.60s/it]   45%|████▍     | 413/921 [49:10<56:28,  6.67s/it] 45%|████▍     | 414/921 [49:19<1:01:25,  7.27s/it] 45%|████▌     | 415/921 [49:27<1:02:05,  7.36s/it] 45%|████▌     | 416/921 [49:34<1:01:01,  7.25s/it] 45%|████▌     | 417/921 [49:41<1:00:11,  7.17s/it] 45%|████▌     | 418/921 [49:47<57:35,  6.87s/it]   45%|████▌     | 419/921 [49:55<1:00:01,  7.17s/it] 46%|████▌     | 420/921 [50:01<57:30,  6.89s/it]                                                   {'loss': 1.2863, 'grad_norm': 0.652671754360199, 'learning_rate': 6.620663938438664e-06, 'epoch': 1.36}
 46%|████▌     | 420/921 [50:01<57:30,  6.89s/it] 46%|████▌     | 421/921 [50:08<58:02,  6.96s/it] 46%|████▌     | 422/921 [50:14<56:37,  6.81s/it] 46%|████▌     | 423/921 [50:21<55:01,  6.63s/it] 46%|████▌     | 424/921 [50:27<55:21,  6.68s/it] 46%|████▌     | 425/921 [50:38<1:05:40,  7.94s/it] 46%|████▋     | 426/921 [50:47<1:07:54,  8.23s/it] 46%|████▋     | 427/921 [50:54<1:03:32,  7.72s/it] 46%|████▋     | 428/921 [51:08<1:19:28,  9.67s/it] 47%|████▋     | 429/921 [51:22<1:30:08, 10.99s/it] 47%|████▋     | 430/921 [51:38<1:41:28, 12.40s/it]                                                   {'loss': 1.2445, 'grad_norm': 0.5197580456733704, 'learning_rate': 6.440073011861425e-06, 'epoch': 1.4}
 47%|████▋     | 430/921 [51:38<1:41:28, 12.40s/it] 47%|████▋     | 431/921 [51:53<1:47:14, 13.13s/it] 47%|████▋     | 432/921 [52:05<1:44:49, 12.86s/it] 47%|████▋     | 433/921 [52:16<1:39:17, 12.21s/it] 47%|████▋     | 434/921 [52:31<1:47:35, 13.26s/it] 47%|████▋     | 435/921 [52:48<1:56:56, 14.44s/it] 47%|████▋     | 436/921 [53:00<1:50:24, 13.66s/it] 47%|████▋     | 437/921 [53:14<1:49:24, 13.56s/it] 48%|████▊     | 438/921 [53:25<1:44:58, 13.04s/it] 48%|████▊     | 439/921 [53:40<1:49:06, 13.58s/it] 48%|████▊     | 440/921 [53:53<1:47:27, 13.41s/it]                                                   {'loss': 1.4041, 'grad_norm': 0.8325708508491516, 'learning_rate': 6.257409218071988e-06, 'epoch': 1.43}
 48%|████▊     | 440/921 [53:53<1:47:27, 13.41s/it] 48%|████▊     | 441/921 [54:03<1:39:36, 12.45s/it] 48%|████▊     | 442/921 [54:17<1:41:17, 12.69s/it] 48%|████▊     | 443/921 [54:27<1:36:15, 12.08s/it] 48%|████▊     | 444/921 [54:39<1:33:59, 11.82s/it] 48%|████▊     | 445/921 [54:53<1:40:33, 12.68s/it] 48%|████▊     | 446/921 [55:10<1:50:55, 14.01s/it] 49%|████▊     | 447/921 [55:22<1:44:28, 13.23s/it] 49%|████▊     | 448/921 [55:29<1:29:11, 11.31s/it] 49%|████▉     | 449/921 [55:35<1:16:25,  9.72s/it] 49%|████▉     | 450/921 [55:44<1:16:15,  9.72s/it]                                                   {'loss': 1.3703, 'grad_norm': 0.5488327145576477, 'learning_rate': 6.072935486648144e-06, 'epoch': 1.46}
 49%|████▉     | 450/921 [55:44<1:16:15,  9.72s/it] 49%|████▉     | 451/921 [55:53<1:13:36,  9.40s/it] 49%|████▉     | 452/921 [56:01<1:09:12,  8.85s/it] 49%|████▉     | 453/921 [56:06<1:01:33,  7.89s/it] 49%|████▉     | 454/921 [56:14<1:00:17,  7.75s/it] 49%|████▉     | 455/921 [56:21<58:35,  7.54s/it]   50%|████▉     | 456/921 [56:27<55:06,  7.11s/it] 50%|████▉     | 457/921 [56:35<56:27,  7.30s/it] 50%|████▉     | 458/921 [56:42<55:57,  7.25s/it] 50%|████▉     | 459/921 [56:48<54:44,  7.11s/it] 50%|████▉     | 460/921 [56:57<57:49,  7.53s/it]                                                 {'loss': 1.3775, 'grad_norm': 0.744675874710083, 'learning_rate': 5.886917352424748e-06, 'epoch': 1.49}
 50%|████▉     | 460/921 [56:57<57:49,  7.53s/it] 50%|█████     | 461/921 [57:05<59:25,  7.75s/it] 50%|█████     | 462/921 [57:14<1:00:44,  7.94s/it] 50%|█████     | 463/921 [57:22<1:01:30,  8.06s/it] 50%|█████     | 464/921 [57:28<57:46,  7.59s/it]   50%|█████     | 465/921 [57:36<56:49,  7.48s/it] 51%|█████     | 466/921 [57:41<52:48,  6.96s/it] 51%|█████     | 467/921 [57:52<1:00:05,  7.94s/it] 51%|█████     | 468/921 [57:58<57:23,  7.60s/it]   51%|█████     | 469/921 [58:05<54:58,  7.30s/it] 51%|█████     | 470/921 [58:12<53:55,  7.17s/it]                                                 {'loss': 1.2823, 'grad_norm': 0.5151727199554443, 'learning_rate': 5.699622573278054e-06, 'epoch': 1.53}
 51%|█████     | 470/921 [58:12<53:55,  7.17s/it] 51%|█████     | 471/921 [58:18<51:00,  6.80s/it] 51%|█████     | 472/921 [58:24<49:31,  6.62s/it] 51%|█████▏    | 473/921 [58:30<47:46,  6.40s/it] 51%|█████▏    | 474/921 [58:37<48:04,  6.45s/it] 52%|█████▏    | 475/921 [58:44<50:55,  6.85s/it] 52%|█████▏    | 476/921 [58:51<50:59,  6.88s/it] 52%|█████▏    | 477/921 [59:00<55:19,  7.48s/it] 52%|█████▏    | 478/921 [59:06<50:41,  6.87s/it] 52%|█████▏    | 479/921 [59:12<49:31,  6.72s/it] 52%|█████▏    | 480/921 [59:18<47:29,  6.46s/it]                                                 {'loss': 1.3993, 'grad_norm': 0.7766989469528198, 'learning_rate': 5.511320744710171e-06, 'epoch': 1.56}
 52%|█████▏    | 480/921 [59:18<47:29,  6.46s/it] 52%|█████▏    | 481/921 [59:24<46:48,  6.38s/it] 52%|█████▏    | 482/921 [59:32<51:09,  6.99s/it] 52%|█████▏    | 483/921 [59:39<49:12,  6.74s/it] 53%|█████▎    | 484/921 [59:47<53:46,  7.38s/it] 53%|█████▎    | 485/921 [59:53<50:38,  6.97s/it] 53%|█████▎    | 486/921 [1:00:00<50:15,  6.93s/it] 53%|█████▎    | 487/921 [1:00:09<54:12,  7.49s/it] 53%|█████▎    | 488/921 [1:00:16<52:15,  7.24s/it] 53%|█████▎    | 489/921 [1:00:21<48:29,  6.73s/it] 53%|█████▎    | 490/921 [1:00:27<45:17,  6.30s/it]                                                   {'loss': 1.3412, 'grad_norm': 0.6519820094108582, 'learning_rate': 5.322282911788416e-06, 'epoch': 1.59}
 53%|█████▎    | 490/921 [1:00:27<45:17,  6.30s/it] 53%|█████▎    | 491/921 [1:00:33<46:15,  6.46s/it] 53%|█████▎    | 492/921 [1:00:39<45:16,  6.33s/it] 54%|█████▎    | 493/921 [1:00:46<46:32,  6.52s/it] 54%|█████▎    | 494/921 [1:00:53<47:05,  6.62s/it] 54%|█████▎    | 495/921 [1:00:59<45:20,  6.39s/it] 54%|█████▍    | 496/921 [1:01:05<44:24,  6.27s/it] 54%|█████▍    | 497/921 [1:01:11<43:19,  6.13s/it] 54%|█████▍    | 498/921 [1:01:17<43:15,  6.14s/it] 54%|█████▍    | 499/921 [1:01:23<42:14,  6.01s/it] 54%|█████▍    | 500/921 [1:01:29<42:51,  6.11s/it]                                                   {'loss': 1.3533, 'grad_norm': 0.6925826072692871, 'learning_rate': 5.132781178998113e-06, 'epoch': 1.62}
 54%|█████▍    | 500/921 [1:01:29<42:51,  6.11s/it][INFO|trainer.py:3942] 2025-04-13 20:11:32,608 >> Saving model checkpoint to saves/llama2-7b/lora/sft-assistant-all/checkpoint-500
[INFO|configuration_utils.py:697] 2025-04-13 20:11:32,630 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 20:11:32,631 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-04-13 20:11:32,751 >> tokenizer config file saved in saves/llama2-7b/lora/sft-assistant-all/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-04-13 20:11:32,751 >> Special tokens file saved in saves/llama2-7b/lora/sft-assistant-all/checkpoint-500/special_tokens_map.json
 54%|█████▍    | 501/921 [1:01:35<42:36,  6.09s/it] 55%|█████▍    | 502/921 [1:01:42<44:17,  6.34s/it] 55%|█████▍    | 503/921 [1:01:51<49:34,  7.12s/it] 55%|█████▍    | 504/921 [1:01:58<49:53,  7.18s/it] 55%|█████▍    | 505/921 [1:02:05<48:50,  7.04s/it] 55%|█████▍    | 506/921 [1:02:10<45:16,  6.55s/it] 55%|█████▌    | 507/921 [1:02:19<48:27,  7.02s/it] 55%|█████▌    | 508/921 [1:02:25<47:34,  6.91s/it] 55%|█████▌    | 509/921 [1:02:32<47:57,  6.98s/it] 55%|█████▌    | 510/921 [1:02:39<47:05,  6.87s/it]                                                   {'loss': 1.366, 'grad_norm': 0.6841617822647095, 'learning_rate': 4.9430883185704796e-06, 'epoch': 1.66}
 55%|█████▌    | 510/921 [1:02:39<47:05,  6.87s/it] 55%|█████▌    | 511/921 [1:02:46<46:45,  6.84s/it] 56%|█████▌    | 512/921 [1:02:51<43:54,  6.44s/it] 56%|█████▌    | 513/921 [1:02:58<44:41,  6.57s/it] 56%|█████▌    | 514/921 [1:03:04<43:57,  6.48s/it] 56%|█████▌    | 515/921 [1:03:11<43:04,  6.36s/it] 56%|█████▌    | 516/921 [1:03:17<42:52,  6.35s/it] 56%|█████▌    | 517/921 [1:03:24<45:15,  6.72s/it] 56%|█████▌    | 518/921 [1:03:34<50:05,  7.46s/it] 56%|█████▋    | 519/921 [1:03:41<50:39,  7.56s/it] 56%|█████▋    | 520/921 [1:03:48<48:55,  7.32s/it]                                                   {'loss': 1.3639, 'grad_norm': 0.7049739360809326, 'learning_rate': 4.753477377849316e-06, 'epoch': 1.69}
 56%|█████▋    | 520/921 [1:03:48<48:55,  7.32s/it] 57%|█████▋    | 521/921 [1:03:56<49:11,  7.38s/it] 57%|█████▋    | 522/921 [1:04:03<48:55,  7.36s/it] 57%|█████▋    | 523/921 [1:04:11<49:12,  7.42s/it] 57%|█████▋    | 524/921 [1:04:20<52:29,  7.93s/it] 57%|█████▋    | 525/921 [1:04:25<48:06,  7.29s/it] 57%|█████▋    | 526/921 [1:04:32<47:04,  7.15s/it] 57%|█████▋    | 527/921 [1:04:40<48:42,  7.42s/it] 57%|█████▋    | 528/921 [1:04:48<49:25,  7.55s/it] 57%|█████▋    | 529/921 [1:04:54<46:51,  7.17s/it] 58%|█████▊    | 530/921 [1:05:04<51:17,  7.87s/it]                                                   {'loss': 1.3179, 'grad_norm': 0.7056111097335815, 'learning_rate': 4.564221286261709e-06, 'epoch': 1.72}
 58%|█████▊    | 530/921 [1:05:04<51:17,  7.87s/it] 58%|█████▊    | 531/921 [1:05:11<49:02,  7.54s/it] 58%|█████▊    | 532/921 [1:05:18<48:37,  7.50s/it] 58%|█████▊    | 533/921 [1:05:25<47:39,  7.37s/it] 58%|█████▊    | 534/921 [1:05:30<42:06,  6.53s/it] 58%|█████▊    | 535/921 [1:05:40<48:12,  7.49s/it] 58%|█████▊    | 536/921 [1:05:46<46:14,  7.21s/it] 58%|█████▊    | 537/921 [1:05:53<46:25,  7.25s/it] 58%|█████▊    | 538/921 [1:06:01<46:59,  7.36s/it] 59%|█████▊    | 539/921 [1:06:07<44:10,  6.94s/it] 59%|█████▊    | 540/921 [1:06:12<41:12,  6.49s/it]                                                   {'loss': 1.2852, 'grad_norm': 0.8867459893226624, 'learning_rate': 4.375592462458477e-06, 'epoch': 1.75}
 59%|█████▊    | 540/921 [1:06:12<41:12,  6.49s/it] 59%|█████▊    | 541/921 [1:06:20<42:47,  6.76s/it] 59%|█████▉    | 542/921 [1:06:27<42:34,  6.74s/it] 59%|█████▉    | 543/921 [1:06:35<45:27,  7.22s/it] 59%|█████▉    | 544/921 [1:06:44<48:18,  7.69s/it] 59%|█████▉    | 545/921 [1:06:52<48:35,  7.75s/it] 59%|█████▉    | 546/921 [1:06:59<48:40,  7.79s/it] 59%|█████▉    | 547/921 [1:07:05<45:21,  7.28s/it] 60%|█████▉    | 548/921 [1:07:13<46:11,  7.43s/it] 60%|█████▉    | 549/921 [1:07:23<50:07,  8.08s/it] 60%|█████▉    | 550/921 [1:07:29<46:48,  7.57s/it]                                                   {'loss': 1.3563, 'grad_norm': 0.44705164432525635, 'learning_rate': 4.1878624221897985e-06, 'epoch': 1.79}
 60%|█████▉    | 550/921 [1:07:29<46:48,  7.57s/it] 60%|█████▉    | 551/921 [1:07:37<47:07,  7.64s/it] 60%|█████▉    | 552/921 [1:07:45<46:50,  7.62s/it] 60%|██████    | 553/921 [1:07:50<42:51,  6.99s/it] 60%|██████    | 554/921 [1:07:55<39:16,  6.42s/it] 60%|██████    | 555/921 [1:08:01<37:57,  6.22s/it] 60%|██████    | 556/921 [1:08:08<40:03,  6.59s/it] 60%|██████    | 557/921 [1:08:15<40:45,  6.72s/it] 61%|██████    | 558/921 [1:08:24<44:48,  7.41s/it] 61%|██████    | 559/921 [1:08:32<44:40,  7.41s/it] 61%|██████    | 560/921 [1:08:37<39:31,  6.57s/it]                                                   {'loss': 1.3063, 'grad_norm': 0.7369788289070129, 'learning_rate': 4.001301387480543e-06, 'epoch': 1.82}
 61%|██████    | 560/921 [1:08:37<39:31,  6.57s/it] 61%|██████    | 561/921 [1:08:44<41:41,  6.95s/it] 61%|██████    | 562/921 [1:08:51<40:37,  6.79s/it] 61%|██████    | 563/921 [1:09:00<44:13,  7.41s/it] 61%|██████    | 564/921 [1:09:06<42:12,  7.09s/it] 61%|██████▏   | 565/921 [1:09:13<41:13,  6.95s/it] 61%|██████▏   | 566/921 [1:09:19<40:35,  6.86s/it] 62%|██████▏   | 567/921 [1:09:27<41:14,  6.99s/it] 62%|██████▏   | 568/921 [1:09:33<40:07,  6.82s/it] 62%|██████▏   | 569/921 [1:09:42<43:40,  7.44s/it] 62%|██████▏   | 570/921 [1:09:49<43:10,  7.38s/it]                                                   {'loss': 1.3525, 'grad_norm': 0.7818019390106201, 'learning_rate': 3.816177897667767e-06, 'epoch': 1.85}
 62%|██████▏   | 570/921 [1:09:49<43:10,  7.38s/it] 62%|██████▏   | 571/921 [1:09:59<47:55,  8.22s/it] 62%|██████▏   | 572/921 [1:10:05<44:00,  7.56s/it] 62%|██████▏   | 573/921 [1:10:15<47:35,  8.21s/it] 62%|██████▏   | 574/921 [1:10:22<44:34,  7.71s/it] 62%|██████▏   | 575/921 [1:10:29<43:10,  7.49s/it] 63%|██████▎   | 576/921 [1:10:35<41:21,  7.19s/it] 63%|██████▎   | 577/921 [1:10:42<40:02,  6.98s/it] 63%|██████▎   | 578/921 [1:10:52<45:11,  7.90s/it] 63%|██████▎   | 579/921 [1:11:01<47:20,  8.31s/it] 63%|██████▎   | 580/921 [1:11:07<43:38,  7.68s/it]                                                   {'loss': 1.3485, 'grad_norm': 0.8802053928375244, 'learning_rate': 3.632758422860335e-06, 'epoch': 1.88}
 63%|██████▎   | 580/921 [1:11:07<43:38,  7.68s/it] 63%|██████▎   | 581/921 [1:11:16<46:05,  8.13s/it] 63%|██████▎   | 582/921 [1:11:23<42:50,  7.58s/it] 63%|██████▎   | 583/921 [1:11:29<40:50,  7.25s/it] 63%|██████▎   | 584/921 [1:11:36<40:22,  7.19s/it] 64%|██████▎   | 585/921 [1:11:44<41:47,  7.46s/it] 64%|██████▎   | 586/921 [1:11:50<39:27,  7.07s/it] 64%|██████▎   | 587/921 [1:11:58<40:33,  7.29s/it] 64%|██████▍   | 588/921 [1:12:06<41:13,  7.43s/it] 64%|██████▍   | 589/921 [1:12:13<40:50,  7.38s/it] 64%|██████▍   | 590/921 [1:12:20<39:19,  7.13s/it]                                                   {'loss': 1.4603, 'grad_norm': 0.8016929626464844, 'learning_rate': 3.4513069803770044e-06, 'epoch': 1.92}
 64%|██████▍   | 590/921 [1:12:20<39:19,  7.13s/it] 64%|██████▍   | 591/921 [1:12:26<38:43,  7.04s/it] 64%|██████▍   | 592/921 [1:12:33<38:32,  7.03s/it] 64%|██████▍   | 593/921 [1:12:40<37:12,  6.81s/it] 64%|██████▍   | 594/921 [1:12:47<37:38,  6.91s/it] 65%|██████▍   | 595/921 [1:12:54<37:56,  6.98s/it] 65%|██████▍   | 596/921 [1:13:00<36:44,  6.78s/it] 65%|██████▍   | 597/921 [1:13:09<38:46,  7.18s/it] 65%|██████▍   | 598/921 [1:13:14<36:16,  6.74s/it] 65%|██████▌   | 599/921 [1:13:21<35:41,  6.65s/it] 65%|██████▌   | 600/921 [1:13:27<34:45,  6.50s/it]                                                   {'loss': 1.3593, 'grad_norm': 1.0582188367843628, 'learning_rate': 3.2720847547151096e-06, 'epoch': 1.95}
 65%|██████▌   | 600/921 [1:13:27<34:45,  6.50s/it] 65%|██████▌   | 601/921 [1:13:34<35:35,  6.67s/it] 65%|██████▌   | 602/921 [1:13:40<35:22,  6.65s/it] 65%|██████▌   | 603/921 [1:13:49<38:25,  7.25s/it] 66%|██████▌   | 604/921 [1:13:55<36:00,  6.82s/it] 66%|██████▌   | 605/921 [1:14:01<34:31,  6.56s/it] 66%|██████▌   | 606/921 [1:14:08<35:07,  6.69s/it] 66%|██████▌   | 607/921 [1:14:16<37:49,  7.23s/it] 66%|██████▌   | 608/921 [1:14:25<39:39,  7.60s/it] 66%|██████▌   | 609/921 [1:14:33<40:16,  7.74s/it] 66%|██████▌   | 610/921 [1:14:40<39:27,  7.61s/it]                                                   {'loss': 1.3147, 'grad_norm': 0.9039961099624634, 'learning_rate': 3.0953497215968665e-06, 'epoch': 1.98}
 66%|██████▌   | 610/921 [1:14:40<39:27,  7.61s/it] 66%|██████▋   | 611/921 [1:14:47<38:35,  7.47s/it] 66%|██████▋   | 612/921 [1:14:57<41:28,  8.05s/it] 67%|██████▋   | 613/921 [1:15:02<36:52,  7.18s/it] 67%|██████▋   | 614/921 [1:15:09<35:58,  7.03s/it] 67%|██████▋   | 615/921 [1:15:17<38:11,  7.49s/it] 67%|██████▋   | 616/921 [1:15:22<33:59,  6.69s/it] 67%|██████▋   | 617/921 [1:15:29<34:14,  6.76s/it] 67%|██████▋   | 618/921 [1:15:34<31:21,  6.21s/it] 67%|██████▋   | 619/921 [1:15:42<34:35,  6.87s/it] 67%|██████▋   | 620/921 [1:15:50<35:51,  7.15s/it]                                                   {'loss': 1.3308, 'grad_norm': 0.7760568857192993, 'learning_rate': 2.921356276634422e-06, 'epoch': 2.01}
 67%|██████▋   | 620/921 [1:15:50<35:51,  7.15s/it] 67%|██████▋   | 621/921 [1:16:00<39:13,  7.84s/it] 68%|██████▊   | 622/921 [1:16:07<38:13,  7.67s/it] 68%|██████▊   | 623/921 [1:16:15<38:30,  7.75s/it] 68%|██████▊   | 624/921 [1:16:21<36:11,  7.31s/it] 68%|██████▊   | 625/921 [1:16:26<32:39,  6.62s/it] 68%|██████▊   | 626/921 [1:16:32<31:55,  6.49s/it] 68%|██████▊   | 627/921 [1:16:38<31:14,  6.38s/it] 68%|██████▊   | 628/921 [1:16:44<30:00,  6.15s/it] 68%|██████▊   | 629/921 [1:16:50<29:48,  6.13s/it] 68%|██████▊   | 630/921 [1:16:56<29:37,  6.11s/it]                                                   {'loss': 1.4188, 'grad_norm': 0.6308320164680481, 'learning_rate': 2.75035486914822e-06, 'epoch': 2.05}
 68%|██████▊   | 630/921 [1:16:56<29:37,  6.11s/it] 69%|██████▊   | 631/921 [1:17:06<35:14,  7.29s/it] 69%|██████▊   | 632/921 [1:17:13<34:21,  7.13s/it] 69%|██████▊   | 633/921 [1:17:20<33:47,  7.04s/it] 69%|██████▉   | 634/921 [1:17:28<35:26,  7.41s/it] 69%|██████▉   | 635/921 [1:17:34<32:56,  6.91s/it] 69%|██████▉   | 636/921 [1:17:42<34:35,  7.28s/it] 69%|██████▉   | 637/921 [1:17:49<33:48,  7.14s/it] 69%|██████▉   | 638/921 [1:17:57<34:51,  7.39s/it] 69%|██████▉   | 639/921 [1:18:03<33:23,  7.11s/it] 69%|██████▉   | 640/921 [1:18:10<32:58,  7.04s/it]                                                   {'loss': 1.3694, 'grad_norm': 0.7068501114845276, 'learning_rate': 2.58259164166568e-06, 'epoch': 2.08}
 69%|██████▉   | 640/921 [1:18:10<32:58,  7.04s/it] 70%|██████▉   | 641/921 [1:18:18<33:46,  7.24s/it] 70%|██████▉   | 642/921 [1:18:26<34:44,  7.47s/it] 70%|██████▉   | 643/921 [1:18:32<32:24,  7.00s/it] 70%|██████▉   | 644/921 [1:18:38<31:50,  6.90s/it] 70%|███████   | 645/921 [1:18:45<31:02,  6.75s/it] 70%|███████   | 646/921 [1:18:50<28:41,  6.26s/it] 70%|███████   | 647/921 [1:18:56<28:06,  6.15s/it] 70%|███████   | 648/921 [1:19:02<27:59,  6.15s/it] 70%|███████   | 649/921 [1:19:10<30:44,  6.78s/it] 71%|███████   | 650/921 [1:19:18<32:40,  7.23s/it]                                                   {'loss': 1.2869, 'grad_norm': 0.3430003225803375, 'learning_rate': 2.4183080756192048e-06, 'epoch': 2.11}
 71%|███████   | 650/921 [1:19:18<32:40,  7.23s/it] 71%|███████   | 651/921 [1:19:25<31:05,  6.91s/it] 71%|███████   | 652/921 [1:19:31<29:57,  6.68s/it] 71%|███████   | 653/921 [1:19:36<28:37,  6.41s/it] 71%|███████   | 654/921 [1:19:42<27:49,  6.25s/it] 71%|███████   | 655/921 [1:19:49<27:41,  6.25s/it] 71%|███████   | 656/921 [1:19:55<27:56,  6.33s/it] 71%|███████▏  | 657/921 [1:20:03<30:34,  6.95s/it] 71%|███████▏  | 658/921 [1:20:10<29:58,  6.84s/it] 72%|███████▏  | 659/921 [1:20:19<32:37,  7.47s/it] 72%|███████▏  | 660/921 [1:20:25<31:07,  7.16s/it]                                                   {'loss': 1.3987, 'grad_norm': 0.9534925222396851, 'learning_rate': 2.2577406437534055e-06, 'epoch': 2.14}
 72%|███████▏  | 660/921 [1:20:25<31:07,  7.16s/it] 72%|███████▏  | 661/921 [1:20:33<31:22,  7.24s/it] 72%|███████▏  | 662/921 [1:20:38<28:59,  6.72s/it] 72%|███████▏  | 663/921 [1:20:45<28:51,  6.71s/it] 72%|███████▏  | 664/921 [1:20:52<28:34,  6.67s/it] 72%|███████▏  | 665/921 [1:20:58<28:12,  6.61s/it] 72%|███████▏  | 666/921 [1:21:04<27:45,  6.53s/it] 72%|███████▏  | 667/921 [1:21:10<26:46,  6.33s/it] 73%|███████▎  | 668/921 [1:21:17<26:37,  6.31s/it] 73%|███████▎  | 669/921 [1:21:23<27:06,  6.46s/it] 73%|███████▎  | 670/921 [1:21:30<27:22,  6.54s/it]                                                   {'loss': 1.3009, 'grad_norm': 0.8745605945587158, 'learning_rate': 2.101120469741962e-06, 'epoch': 2.18}
 73%|███████▎  | 670/921 [1:21:30<27:22,  6.54s/it] 73%|███████▎  | 671/921 [1:21:37<28:08,  6.76s/it] 73%|███████▎  | 672/921 [1:21:43<27:06,  6.53s/it] 73%|███████▎  | 673/921 [1:21:51<28:05,  6.80s/it] 73%|███████▎  | 674/921 [1:21:59<29:43,  7.22s/it] 73%|███████▎  | 675/921 [1:22:06<29:23,  7.17s/it] 73%|███████▎  | 676/921 [1:22:12<27:50,  6.82s/it] 74%|███████▎  | 677/921 [1:22:18<26:32,  6.53s/it] 74%|███████▎  | 678/921 [1:22:26<27:55,  6.90s/it] 74%|███████▎  | 679/921 [1:22:33<28:13,  7.00s/it] 74%|███████▍  | 680/921 [1:22:40<28:48,  7.17s/it]                                                   {'loss': 1.2213, 'grad_norm': 0.9698092937469482, 'learning_rate': 1.948672995503998e-06, 'epoch': 2.21}
 74%|███████▍  | 680/921 [1:22:40<28:48,  7.17s/it] 74%|███████▍  | 681/921 [1:22:49<30:41,  7.67s/it] 74%|███████▍  | 682/921 [1:22:55<28:17,  7.10s/it] 74%|███████▍  | 683/921 [1:23:04<30:25,  7.67s/it] 74%|███████▍  | 684/921 [1:23:12<31:01,  7.85s/it] 74%|███████▍  | 685/921 [1:23:18<28:36,  7.27s/it] 74%|███████▍  | 686/921 [1:23:24<26:34,  6.78s/it] 75%|███████▍  | 687/921 [1:23:33<28:33,  7.32s/it] 75%|███████▍  | 688/921 [1:23:38<26:18,  6.77s/it] 75%|███████▍  | 689/921 [1:23:46<28:09,  7.28s/it] 75%|███████▍  | 690/921 [1:23:52<26:20,  6.84s/it]                                                   {'loss': 1.2744, 'grad_norm': 0.7880718111991882, 'learning_rate': 1.8006176566989064e-06, 'epoch': 2.24}
 75%|███████▍  | 690/921 [1:23:52<26:20,  6.84s/it] 75%|███████▌  | 691/921 [1:24:00<27:25,  7.15s/it] 75%|███████▌  | 692/921 [1:24:09<28:42,  7.52s/it] 75%|███████▌  | 693/921 [1:24:17<30:00,  7.90s/it] 75%|███████▌  | 694/921 [1:24:24<28:21,  7.49s/it] 75%|███████▌  | 695/921 [1:24:31<27:30,  7.30s/it] 76%|███████▌  | 696/921 [1:24:37<26:09,  6.97s/it] 76%|███████▌  | 697/921 [1:24:43<25:09,  6.74s/it] 76%|███████▌  | 698/921 [1:24:53<28:09,  7.57s/it] 76%|███████▌  | 699/921 [1:24:59<26:55,  7.28s/it] 76%|███████▌  | 700/921 [1:25:07<26:49,  7.28s/it]                                                   {'loss': 1.3071, 'grad_norm': 0.4124126434326172, 'learning_rate': 1.6571675668666742e-06, 'epoch': 2.27}
 76%|███████▌  | 700/921 [1:25:07<26:49,  7.28s/it] 76%|███████▌  | 701/921 [1:25:12<24:15,  6.61s/it] 76%|███████▌  | 702/921 [1:25:20<26:22,  7.23s/it] 76%|███████▋  | 703/921 [1:25:30<28:52,  7.95s/it] 76%|███████▋  | 704/921 [1:25:37<27:32,  7.62s/it] 77%|███████▋  | 705/921 [1:25:44<26:50,  7.46s/it] 77%|███████▋  | 706/921 [1:25:51<26:21,  7.35s/it] 77%|███████▋  | 707/921 [1:25:58<26:17,  7.37s/it] 77%|███████▋  | 708/921 [1:26:07<27:28,  7.74s/it] 77%|███████▋  | 709/921 [1:26:15<27:45,  7.86s/it] 77%|███████▋  | 710/921 [1:26:23<27:44,  7.89s/it]                                                   {'loss': 1.3015, 'grad_norm': 0.8331634402275085, 'learning_rate': 1.5185292106683963e-06, 'epoch': 2.31}
 77%|███████▋  | 710/921 [1:26:23<27:44,  7.89s/it] 77%|███████▋  | 711/921 [1:26:30<26:38,  7.61s/it] 77%|███████▋  | 712/921 [1:26:35<24:03,  6.91s/it] 77%|███████▋  | 713/921 [1:26:42<23:34,  6.80s/it] 78%|███████▊  | 714/921 [1:26:51<25:44,  7.46s/it] 78%|███████▊  | 715/921 [1:27:00<27:20,  7.96s/it] 78%|███████▊  | 716/921 [1:27:10<28:54,  8.46s/it] 78%|███████▊  | 717/921 [1:27:17<27:58,  8.23s/it] 78%|███████▊  | 718/921 [1:27:24<26:38,  7.87s/it] 78%|███████▊  | 719/921 [1:27:30<23:52,  7.09s/it] 78%|███████▊  | 720/921 [1:27:38<25:09,  7.51s/it]                                                   {'loss': 1.3651, 'grad_norm': 0.7806615829467773, 'learning_rate': 1.384902146668497e-06, 'epoch': 2.34}
 78%|███████▊  | 720/921 [1:27:38<25:09,  7.51s/it] 78%|███████▊  | 721/921 [1:27:45<24:52,  7.46s/it] 78%|███████▊  | 722/921 [1:27:52<23:33,  7.10s/it] 79%|███████▊  | 723/921 [1:27:58<23:01,  6.98s/it] 79%|███████▊  | 724/921 [1:28:05<22:37,  6.89s/it] 79%|███████▊  | 725/921 [1:28:12<23:00,  7.04s/it] 79%|███████▉  | 726/921 [1:28:19<22:47,  7.01s/it] 79%|███████▉  | 727/921 [1:28:28<23:55,  7.40s/it] 79%|███████▉  | 728/921 [1:28:33<22:07,  6.88s/it] 79%|███████▉  | 729/921 [1:28:41<22:30,  7.03s/it] 79%|███████▉  | 730/921 [1:28:49<23:09,  7.28s/it]                                                   {'loss': 1.3364, 'grad_norm': 0.9404022097587585, 'learning_rate': 1.2564787200865226e-06, 'epoch': 2.37}
 79%|███████▉  | 730/921 [1:28:49<23:09,  7.28s/it] 79%|███████▉  | 731/921 [1:28:56<23:36,  7.45s/it] 79%|███████▉  | 732/921 [1:29:03<22:19,  7.09s/it] 80%|███████▉  | 733/921 [1:29:10<21:58,  7.02s/it] 80%|███████▉  | 734/921 [1:29:15<20:33,  6.59s/it] 80%|███████▉  | 735/921 [1:29:21<19:29,  6.29s/it] 80%|███████▉  | 736/921 [1:29:30<21:45,  7.06s/it] 80%|████████  | 737/921 [1:29:36<21:29,  7.01s/it] 80%|████████  | 738/921 [1:29:45<22:31,  7.39s/it] 80%|████████  | 739/921 [1:29:52<22:26,  7.40s/it] 80%|████████  | 740/921 [1:30:00<22:39,  7.51s/it]                                                   {'loss': 1.3398, 'grad_norm': 0.839165210723877, 'learning_rate': 1.133443785931937e-06, 'epoch': 2.4}
 80%|████████  | 740/921 [1:30:00<22:39,  7.51s/it] 80%|████████  | 741/921 [1:30:06<21:17,  7.10s/it] 81%|████████  | 742/921 [1:30:14<22:10,  7.43s/it] 81%|████████  | 743/921 [1:30:22<22:42,  7.65s/it] 81%|████████  | 744/921 [1:30:33<24:54,  8.44s/it] 81%|████████  | 745/921 [1:30:39<22:25,  7.65s/it] 81%|████████  | 746/921 [1:30:45<21:35,  7.40s/it] 81%|████████  | 747/921 [1:30:54<22:07,  7.63s/it] 81%|████████  | 748/921 [1:31:05<25:25,  8.82s/it] 81%|████████▏ | 749/921 [1:31:15<26:11,  9.13s/it] 81%|████████▏ | 750/921 [1:31:23<24:56,  8.75s/it]                                                   {'loss': 1.282, 'grad_norm': 0.45094653964042664, 'learning_rate': 1.0159744429204776e-06, 'epoch': 2.44}
 81%|████████▏ | 750/921 [1:31:23<24:56,  8.75s/it] 82%|████████▏ | 751/921 [1:31:28<21:46,  7.69s/it] 82%|████████▏ | 752/921 [1:31:35<20:42,  7.35s/it] 82%|████████▏ | 753/921 [1:31:43<21:23,  7.64s/it] 82%|████████▏ | 754/921 [1:31:48<19:26,  6.99s/it] 82%|████████▏ | 755/921 [1:31:55<19:23,  7.01s/it] 82%|████████▏ | 756/921 [1:32:02<19:07,  6.95s/it] 82%|████████▏ | 757/921 [1:32:08<18:17,  6.69s/it] 82%|████████▏ | 758/921 [1:32:15<18:32,  6.83s/it] 82%|████████▏ | 759/921 [1:32:23<18:55,  7.01s/it] 83%|████████▎ | 760/921 [1:32:30<18:42,  6.97s/it]                                                   {'loss': 1.2653, 'grad_norm': 0.7154667377471924, 'learning_rate': 9.042397785550405e-07, 'epoch': 2.47}
 83%|████████▎ | 760/921 [1:32:30<18:42,  6.97s/it] 83%|████████▎ | 761/921 [1:32:36<17:42,  6.64s/it] 83%|████████▎ | 762/921 [1:32:42<17:14,  6.50s/it] 83%|████████▎ | 763/921 [1:32:50<18:18,  6.95s/it] 83%|████████▎ | 764/921 [1:32:56<17:46,  6.79s/it] 83%|████████▎ | 765/921 [1:33:05<19:19,  7.43s/it] 83%|████████▎ | 766/921 [1:33:11<17:38,  6.83s/it] 83%|████████▎ | 767/921 [1:33:20<19:17,  7.51s/it] 83%|████████▎ | 768/921 [1:33:25<17:25,  6.83s/it] 83%|████████▎ | 769/921 [1:33:32<17:22,  6.86s/it] 84%|████████▎ | 770/921 [1:33:41<18:37,  7.40s/it]                                                   {'loss': 1.36, 'grad_norm': 0.7503601908683777, 'learning_rate': 7.984006257380727e-07, 'epoch': 2.5}
 84%|████████▎ | 770/921 [1:33:41<18:37,  7.40s/it] 84%|████████▎ | 771/921 [1:33:48<18:47,  7.52s/it] 84%|████████▍ | 772/921 [1:33:56<18:43,  7.54s/it] 84%|████████▍ | 773/921 [1:34:03<18:05,  7.34s/it] 84%|████████▍ | 774/921 [1:34:12<19:19,  7.89s/it] 84%|████████▍ | 775/921 [1:34:19<18:33,  7.63s/it] 84%|████████▍ | 776/921 [1:34:26<18:14,  7.55s/it] 84%|████████▍ | 777/921 [1:34:33<17:14,  7.18s/it] 84%|████████▍ | 778/921 [1:34:39<16:11,  6.79s/it] 85%|████████▍ | 779/921 [1:34:45<15:39,  6.62s/it] 85%|████████▍ | 780/921 [1:34:51<15:22,  6.55s/it]                                                   {'loss': 1.3278, 'grad_norm': 1.0580949783325195, 'learning_rate': 6.98609331265766e-07, 'epoch': 2.53}
 85%|████████▍ | 780/921 [1:34:51<15:22,  6.55s/it] 85%|████████▍ | 781/921 [1:34:59<16:08,  6.92s/it] 85%|████████▍ | 782/921 [1:35:06<15:56,  6.88s/it] 85%|████████▌ | 783/921 [1:35:12<15:20,  6.67s/it] 85%|████████▌ | 784/921 [1:35:18<15:01,  6.58s/it] 85%|████████▌ | 785/921 [1:35:25<15:07,  6.67s/it] 85%|████████▌ | 786/921 [1:35:31<14:31,  6.46s/it] 85%|████████▌ | 787/921 [1:35:39<15:12,  6.81s/it] 86%|████████▌ | 788/921 [1:35:45<14:46,  6.66s/it] 86%|████████▌ | 789/921 [1:35:52<14:57,  6.80s/it] 86%|████████▌ | 790/921 [1:35:58<14:29,  6.64s/it]                                                   {'loss': 1.3551, 'grad_norm': 0.7714687585830688, 'learning_rate': 6.050095365373204e-07, 'epoch': 2.57}
 86%|████████▌ | 790/921 [1:35:58<14:29,  6.64s/it] 86%|████████▌ | 791/921 [1:36:05<14:13,  6.57s/it] 86%|████████▌ | 792/921 [1:36:10<13:28,  6.27s/it] 86%|████████▌ | 793/921 [1:36:17<13:17,  6.23s/it] 86%|████████▌ | 794/921 [1:36:24<14:15,  6.73s/it] 86%|████████▋ | 795/921 [1:36:31<13:54,  6.62s/it] 86%|████████▋ | 796/921 [1:36:41<15:42,  7.54s/it] 87%|████████▋ | 797/921 [1:36:47<14:55,  7.23s/it] 87%|████████▋ | 798/921 [1:36:54<14:34,  7.11s/it] 87%|████████▋ | 799/921 [1:37:03<15:37,  7.69s/it] 87%|████████▋ | 800/921 [1:37:11<15:55,  7.90s/it]                                                   {'loss': 1.2985, 'grad_norm': 0.8444893956184387, 'learning_rate': 5.177359707949076e-07, 'epoch': 2.6}
 87%|████████▋ | 800/921 [1:37:11<15:55,  7.90s/it] 87%|████████▋ | 801/921 [1:37:20<16:05,  8.04s/it] 87%|████████▋ | 802/921 [1:37:30<17:21,  8.75s/it] 87%|████████▋ | 803/921 [1:37:38<16:30,  8.40s/it] 87%|████████▋ | 804/921 [1:37:48<17:22,  8.91s/it] 87%|████████▋ | 805/921 [1:37:54<15:28,  8.01s/it] 88%|████████▊ | 806/921 [1:38:00<14:11,  7.40s/it] 88%|████████▊ | 807/921 [1:38:06<13:43,  7.22s/it] 88%|████████▊ | 808/921 [1:38:13<13:26,  7.14s/it] 88%|████████▊ | 809/921 [1:38:22<14:24,  7.72s/it] 88%|████████▊ | 810/921 [1:38:29<13:44,  7.43s/it]                                                   {'loss': 1.3456, 'grad_norm': 0.6018692255020142, 'learning_rate': 4.369142571919599e-07, 'epoch': 2.63}
 88%|████████▊ | 810/921 [1:38:29<13:44,  7.43s/it] 88%|████████▊ | 811/921 [1:38:36<13:23,  7.31s/it] 88%|████████▊ | 812/921 [1:38:44<13:31,  7.44s/it] 88%|████████▊ | 813/921 [1:38:51<13:25,  7.45s/it] 88%|████████▊ | 814/921 [1:38:57<12:27,  6.98s/it] 88%|████████▊ | 815/921 [1:39:05<12:28,  7.06s/it] 89%|████████▊ | 816/921 [1:39:11<12:08,  6.94s/it] 89%|████████▊ | 817/921 [1:39:18<12:00,  6.93s/it] 89%|████████▉ | 818/921 [1:39:24<11:13,  6.54s/it] 89%|████████▉ | 819/921 [1:39:32<11:48,  6.94s/it] 89%|████████▉ | 820/921 [1:39:39<12:03,  7.16s/it]                                                   {'loss': 1.2689, 'grad_norm': 0.8169671297073364, 'learning_rate': 3.626607319689168e-07, 'epoch': 2.66}
 89%|████████▉ | 820/921 [1:39:39<12:03,  7.16s/it] 89%|████████▉ | 821/921 [1:39:47<12:02,  7.22s/it] 89%|████████▉ | 822/921 [1:39:53<11:26,  6.93s/it] 89%|████████▉ | 823/921 [1:39:59<11:04,  6.78s/it] 89%|████████▉ | 824/921 [1:40:08<11:45,  7.27s/it] 90%|████████▉ | 825/921 [1:40:17<12:29,  7.80s/it] 90%|████████▉ | 826/921 [1:40:22<11:07,  7.03s/it] 90%|████████▉ | 827/921 [1:40:31<11:41,  7.46s/it] 90%|████████▉ | 828/921 [1:40:38<11:45,  7.58s/it] 90%|█████████ | 829/921 [1:40:43<10:18,  6.73s/it] 90%|█████████ | 830/921 [1:40:51<10:31,  6.94s/it]                                                   {'loss': 1.2917, 'grad_norm': 0.41607290506362915, 'learning_rate': 2.9508227699673873e-07, 'epoch': 2.7}
 90%|█████████ | 830/921 [1:40:51<10:31,  6.94s/it] 90%|█████████ | 831/921 [1:40:58<10:27,  6.98s/it] 90%|█████████ | 832/921 [1:41:07<11:21,  7.66s/it] 90%|█████████ | 833/921 [1:41:15<11:14,  7.67s/it] 91%|█████████ | 834/921 [1:41:23<11:34,  7.98s/it] 91%|█████████ | 835/921 [1:41:30<10:57,  7.64s/it] 91%|█████████ | 836/921 [1:41:39<11:20,  8.01s/it] 91%|█████████ | 837/921 [1:41:45<10:16,  7.34s/it] 91%|█████████ | 838/921 [1:41:50<09:13,  6.67s/it] 91%|█████████ | 839/921 [1:41:59<09:56,  7.28s/it] 91%|█████████ | 840/921 [1:42:05<09:23,  6.95s/it]                                                   {'loss': 1.2231, 'grad_norm': 0.689877986907959, 'learning_rate': 2.3427616592919587e-07, 'epoch': 2.73}
 91%|█████████ | 840/921 [1:42:05<09:23,  6.95s/it] 91%|█████████▏| 841/921 [1:42:13<09:40,  7.26s/it] 91%|█████████▏| 842/921 [1:42:21<09:53,  7.51s/it] 92%|█████████▏| 843/921 [1:42:27<09:11,  7.08s/it] 92%|█████████▏| 844/921 [1:42:37<10:15,  8.00s/it] 92%|█████████▏| 845/921 [1:42:44<09:34,  7.56s/it] 92%|█████████▏| 846/921 [1:42:50<09:03,  7.25s/it] 92%|█████████▏| 847/921 [1:42:58<09:13,  7.48s/it] 92%|█████████▏| 848/921 [1:43:05<08:49,  7.25s/it] 92%|█████████▏| 849/921 [1:43:11<08:17,  6.91s/it] 92%|█████████▏| 850/921 [1:43:19<08:44,  7.39s/it]                                                   {'loss': 1.3974, 'grad_norm': 0.8361374139785767, 'learning_rate': 1.803299241854156e-07, 'epoch': 2.76}
 92%|█████████▏| 850/921 [1:43:19<08:44,  7.39s/it] 92%|█████████▏| 851/921 [1:43:26<08:13,  7.06s/it] 93%|█████████▎| 852/921 [1:43:32<07:58,  6.94s/it] 93%|█████████▎| 853/921 [1:43:39<07:50,  6.92s/it] 93%|█████████▎| 854/921 [1:43:46<07:40,  6.88s/it] 93%|█████████▎| 855/921 [1:43:52<07:19,  6.66s/it] 93%|█████████▎| 856/921 [1:44:03<08:31,  7.87s/it] 93%|█████████▎| 857/921 [1:44:10<07:59,  7.50s/it] 93%|█████████▎| 858/921 [1:44:16<07:32,  7.18s/it] 93%|█████████▎| 859/921 [1:44:24<07:40,  7.43s/it] 93%|█████████▎| 860/921 [1:44:32<07:39,  7.54s/it]                                                   {'loss': 1.3603, 'grad_norm': 0.6574409604072571, 'learning_rate': 1.3332120296419727e-07, 'epoch': 2.79}
 93%|█████████▎| 860/921 [1:44:32<07:39,  7.54s/it] 93%|█████████▎| 861/921 [1:44:38<07:15,  7.25s/it] 94%|█████████▎| 862/921 [1:44:46<07:12,  7.32s/it] 94%|█████████▎| 863/921 [1:44:52<06:40,  6.90s/it] 94%|█████████▍| 864/921 [1:44:57<06:03,  6.38s/it] 94%|█████████▍| 865/921 [1:45:06<06:35,  7.07s/it] 94%|█████████▍| 866/921 [1:45:12<06:12,  6.77s/it] 94%|█████████▍| 867/921 [1:45:19<06:07,  6.80s/it] 94%|█████████▍| 868/921 [1:45:25<05:57,  6.74s/it] 94%|█████████▍| 869/921 [1:45:33<06:02,  6.97s/it] 94%|█████████▍| 870/921 [1:45:41<06:12,  7.30s/it]                                                   {'loss': 1.4597, 'grad_norm': 0.7992119789123535, 'learning_rate': 9.33176674714753e-08, 'epoch': 2.83}
 94%|█████████▍| 870/921 [1:45:41<06:12,  7.30s/it] 95%|█████████▍| 871/921 [1:45:48<06:05,  7.30s/it] 95%|█████████▍| 872/921 [1:45:56<06:01,  7.38s/it] 95%|█████████▍| 873/921 [1:46:03<05:57,  7.46s/it] 95%|█████████▍| 874/921 [1:46:09<05:31,  7.05s/it] 95%|█████████▌| 875/921 [1:46:17<05:31,  7.20s/it] 95%|█████████▌| 876/921 [1:46:24<05:28,  7.30s/it] 95%|█████████▌| 877/921 [1:46:33<05:36,  7.66s/it] 95%|█████████▌| 878/921 [1:46:40<05:20,  7.45s/it] 95%|█████████▌| 879/921 [1:46:47<05:07,  7.31s/it] 96%|█████████▌| 880/921 [1:46:54<04:52,  7.13s/it]                                                   {'loss': 1.3767, 'grad_norm': 0.7970294952392578, 'learning_rate': 6.037689952179071e-08, 'epoch': 2.86}
 96%|█████████▌| 880/921 [1:46:54<04:52,  7.13s/it] 96%|█████████▌| 881/921 [1:47:01<04:45,  7.13s/it] 96%|█████████▌| 882/921 [1:47:09<04:46,  7.35s/it] 96%|█████████▌| 883/921 [1:47:16<04:35,  7.24s/it] 96%|█████████▌| 884/921 [1:47:23<04:34,  7.42s/it] 96%|█████████▌| 885/921 [1:47:29<04:06,  6.85s/it] 96%|█████████▌| 886/921 [1:47:36<04:07,  7.06s/it] 96%|█████████▋| 887/921 [1:47:44<04:04,  7.20s/it] 96%|█████████▋| 888/921 [1:47:51<03:58,  7.24s/it] 97%|█████████▋| 889/921 [1:47:57<03:32,  6.65s/it] 97%|█████████▋| 890/921 [1:48:03<03:24,  6.61s/it]                                                   {'loss': 1.3717, 'grad_norm': 0.9517481923103333, 'learning_rate': 3.454631465398373e-08, 'epoch': 2.89}
 97%|█████████▋| 890/921 [1:48:03<03:24,  6.61s/it] 97%|█████████▋| 891/921 [1:48:10<03:18,  6.60s/it] 97%|█████████▋| 892/921 [1:48:17<03:14,  6.72s/it] 97%|█████████▋| 893/921 [1:48:23<03:08,  6.72s/it] 97%|█████████▋| 894/921 [1:48:30<02:59,  6.65s/it] 97%|█████████▋| 895/921 [1:48:37<02:54,  6.70s/it] 97%|█████████▋| 896/921 [1:48:42<02:33,  6.15s/it] 97%|█████████▋| 897/921 [1:48:49<02:38,  6.60s/it] 98%|█████████▊| 898/921 [1:48:56<02:32,  6.64s/it] 98%|█████████▊| 899/921 [1:49:06<02:49,  7.70s/it] 98%|█████████▊| 900/921 [1:49:13<02:38,  7.57s/it]                                                   {'loss': 1.3077, 'grad_norm': 0.8532823920249939, 'learning_rate': 1.5863093880408852e-08, 'epoch': 2.92}
 98%|█████████▊| 900/921 [1:49:13<02:38,  7.57s/it] 98%|█████████▊| 901/921 [1:49:20<02:27,  7.38s/it] 98%|█████████▊| 902/921 [1:49:27<02:13,  7.04s/it] 98%|█████████▊| 903/921 [1:49:37<02:24,  8.02s/it] 98%|█████████▊| 904/921 [1:49:44<02:11,  7.73s/it] 98%|█████████▊| 905/921 [1:49:50<01:55,  7.22s/it] 98%|█████████▊| 906/921 [1:49:56<01:43,  6.88s/it] 98%|█████████▊| 907/921 [1:50:03<01:37,  6.95s/it] 99%|█████████▊| 908/921 [1:50:09<01:26,  6.66s/it] 99%|█████████▊| 909/921 [1:50:15<01:18,  6.54s/it] 99%|█████████▉| 910/921 [1:50:25<01:20,  7.33s/it]                                                   {'loss': 1.3973, 'grad_norm': 0.7955302596092224, 'learning_rate': 4.354130167908976e-09, 'epoch': 2.96}
 99%|█████████▉| 910/921 [1:50:25<01:20,  7.33s/it] 99%|█████████▉| 911/921 [1:50:33<01:15,  7.56s/it] 99%|█████████▉| 912/921 [1:50:40<01:06,  7.44s/it] 99%|█████████▉| 913/921 [1:50:45<00:55,  6.88s/it] 99%|█████████▉| 914/921 [1:50:53<00:49,  7.10s/it] 99%|█████████▉| 915/921 [1:50:59<00:40,  6.82s/it] 99%|█████████▉| 916/921 [1:51:05<00:32,  6.59s/it]100%|█████████▉| 917/921 [1:51:14<00:28,  7.21s/it]100%|█████████▉| 918/921 [1:51:20<00:20,  6.92s/it]100%|█████████▉| 919/921 [1:51:28<00:14,  7.25s/it]100%|█████████▉| 920/921 [1:51:36<00:07,  7.46s/it]                                                   {'loss': 1.3266, 'grad_norm': 0.9339495301246643, 'learning_rate': 3.5989727593110924e-11, 'epoch': 2.99}
100%|█████████▉| 920/921 [1:51:36<00:07,  7.46s/it]100%|██████████| 921/921 [1:51:42<00:00,  6.93s/it][INFO|trainer.py:3942] 2025-04-13 21:01:45,311 >> Saving model checkpoint to saves/llama2-7b/lora/sft-assistant-all/checkpoint-921
[INFO|configuration_utils.py:697] 2025-04-13 21:01:45,330 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 21:01:45,331 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-04-13 21:01:45,452 >> tokenizer config file saved in saves/llama2-7b/lora/sft-assistant-all/checkpoint-921/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-04-13 21:01:45,452 >> Special tokens file saved in saves/llama2-7b/lora/sft-assistant-all/checkpoint-921/special_tokens_map.json
[INFO|trainer.py:2657] 2025-04-13 21:01:45,677 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 6702.6955, 'train_samples_per_second': 4.407, 'train_steps_per_second': 0.137, 'train_loss': 1.3999894625718639, 'epoch': 2.99}
100%|██████████| 921/921 [1:51:42<00:00,  6.93s/it]100%|██████████| 921/921 [1:51:42<00:00,  7.28s/it]
[INFO|trainer.py:3942] 2025-04-13 21:01:45,718 >> Saving model checkpoint to saves/llama2-7b/lora/sft-assistant-all
[INFO|configuration_utils.py:697] 2025-04-13 21:01:45,746 >> loading configuration file /data/zli/workspace/zli_work/DSAA6000Q_ass3/model/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:771] 2025-04-13 21:01:45,746 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2500] 2025-04-13 21:01:45,879 >> tokenizer config file saved in saves/llama2-7b/lora/sft-assistant-all/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-04-13 21:01:45,880 >> Special tokens file saved in saves/llama2-7b/lora/sft-assistant-all/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.9911
  total_flos               = 655648291GF
  train_loss               =         1.4
  train_runtime            =  1:51:42.69
  train_samples_per_second =       4.407
  train_steps_per_second   =       0.137
Figure saved at: saves/llama2-7b/lora/sft-assistant-all/training_loss.png
[WARNING|2025-04-13 21:01:46] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-04-13 21:01:46] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:449] 2025-04-13 21:01:46,038 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
